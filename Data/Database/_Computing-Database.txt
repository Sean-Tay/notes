Database
	Abbreviations: 
		DBMS:					DataBase Management System.

		CMDB: 					Configuration Management Database.

	Description: 
		A Database is an Organized, Long-Term Collection of Data, generally Stored, Managed and Accessed via some System.
	
	Resources:
	
	Concepts:
		Data Types:
			Structured Data:	Data whose Elements are Directly Addressible (i.e. Tabular or Array-like Data).
			Semi-Structured Data: Data whose Elements may not be Directly Addressible, but still contain other forms of Organizational Properties (i.e. Tagged Data: XML).
			Unstructured Data:	Data which is currently observed to have no immediately helpful Pre-Defined Format (i.e. Binary Data: PDFs).

		Attribute / Key / Field: Refers to a characteristic of some type of Entity.
		Record:					A collection of Attributes pertaining to some instance of an Entity.
		Collection:				A group of Records.

		Transaction:			A Unit of Work done / Measure of Change performed within a DBMS.
		Heterogenous Transaction: Transactions implemented using the generic XA API that would work for all Databases and setups.
		Internal Transaction: 	Transactions implemented using the specific Database Software's API that would be able to leverage on stronger characteristics of the Database being used.

		Operation:				A Statement within a Transaction.
		ACID:					A set of desirable properties to apply to a Transaction when interacting with a Database.

		Serializability: When the history of Operations being executed concurrently for one or more Objects is the same as one where the Operations are executed sequentially (i.e. no interleaving of Operations).
			Serialization Anomaly: The final Database State that is the result of successfully commiting a group of Transactions is inconsistent with any of all possible Database States arising from all possible orderings of running those Transactions sequentially.

			Strict Serializability: A combination of both Linearizability and Serializability. When the history of Operations being executed concurrently for one or more Objects is the same as one where the Operations are executed sequentially, and in the submitted order (i.e. no interleaving of Operations + correct order of Operations).

		Record-Level Lock: 		A Lock that lasts for as long as the interaction of one Record.
		Operation-Level Lock: 	A Lock that lasts for as long as one Operation within a Transaction.
		Transaction-Level Lock: A Lock that lasts until the completion or abortion of the enclosing Transaction.

		Shared-Mode Lock:		A Lock that allows concurrent Read Access but not Write Access. Obtainable when the involved subject has no Exclusive-Mode Lock active.
		Exclusive-Mode Lock: 	A Lock that allows for both Read and Write Access. Only one Transaction may hold this at any time, and only if the involved subject has no other existing Locks active.

		Record Lock: 			A Lock on a Record.
		Predicate Lock:			A Lock on a potential range of Records.

		Multiversion Concurrency Control: Generates a new version of an Object for each Write, and maintaining the last few relevant versions of the Object for Read Operations.

		Forward Index:			A Data Structure that maps the Record to the Record's Content.
		Inverted Index: 		An additional Data Structure (separate from the Collection) that allows for quick access of a Full Record on Disk via a defined subset of the Record's Content.
			Primary Index / Clustered Index: Each Index Entry is unique (1:1 relation to the Full Records) as it includes the Primary Key of the Records.
			Secondary Index:	Index Entries do not include the Primary Key of the Records - each Index Entry may thus track more than one Record.

		Sparse Index:			Contains a mapping only for every n Records in the Collection to the Disk Location. Typically used in Multi-Level Indexing (i.e further Indexing of a Dense or overly Large Index).
		Dense Index:			Contains a mapping for every Record in the Collection to the Disk Location.

		Database Partitioning:
			Horizontal Partitioning: Splitting a Collection into smaller non-overlapping Partitions and storing them in separate Nodes (i.e. Sharding). Each Node may contain more than one Partition.
			Vertical Partitioning: Different Record Attributes into Separate Collections (i.e. Normalization or simple Record Splitting).

		Hotspot:				A Node in a Database Cluster that receives a higher amount of Traffic and Load compared to others due to sub-optimal Partitioning.

		CAP Theorem: 			Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3).
			Consistency:		Degree at which Data is the same throughout the Database.
				Consistency Model: Defines a contract in implementation to guarantee certain properties of Reads / Writes to Data in a Database. One or more can be implemented to achieve greater Consistency.
				
				Linearizability: An Operation (or a set of Operations) on a Object is seen as Linearizable when they can be considered completely executed and processed within an instantaneous point in time (with respect to other Operations).

			Availability:		Whether or not every Request for Reading / Writing Data is successful.
			Partitioning Tolerance: Enforces that the Database can still operate even if parts of it are unreachable in the Network.

		Replication:			Having Redundant Copies of the Data in a Database stored in multiple Nodes so that if one fails, dependent Applications can still use the other Nodes for Data Retrieval, achieving Reliability for the Distributed System. Also helps to reduce operational load per Instance.
		Replica:				A Node within a Replicated Database.

		Quorum:					A majority of Nodes that agree on something (requires total number of Nodes to be odd). It is impossible for two different Quorums to disagree on something, as the Quorums would have overlapping Nodes by definition.

	Details:
		ACID:					A set of Properties for Operations on a Database to guarantee so as to avoid Erroneous Data.
			Atomicity:			Transactions containing multiple Operations will either succeed as a whole, or not be processed at all.

			Consistency:		Transactions can only bring the Database from one correct State to another, respecting Constraints, Cascades and Triggers within affected Data. This is related to Data Consistency from the CAP Theorem, but not exactly the same.

			Isolation: 			The degree by which Transactions on a Database can assume that they would be unaffected by other concurrently executing Transactions, definable with the types of phenomena that can be observed during overlapping Transactions. The highest degree mandates that the end results of each Transaction can only be one of several, all of which arise from completely non-overlapping Transactions.
				Concurrency Bugs:
					Dirty Operations: Incorrect behaviour stemming from uncommitted Data.
						Writes: A Transaction overwrites committed Data from another concurrently executed Transaction (started later and completed earlier) with uncommitted Data. Uncommitted Data should be able to be subsequently rolled back, but the first Transaction will not be able to roll back without removing the committed Data. Solvable via Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

						Reads: A Transaction sees uncommitted Data from another concurrently executing Transaction. Uncommitted Data can be subsequently rolled back, which leaves the first Transaction with incorrect Data. Solvable via Operation-Level Shared-Mode Record Locks w.r.t each Read Operation, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

					Incorrect behaviour stemming from stale Read Data whose contents were updated whilst the Transaction was paused:
						Fuzzy Reads / Non-Repeatable Reads: A Transaction re-Reads previously read Data and sees that the content within the Read Records has changed. Solvable via Transaction-Level Shared-Mode Record Locks and Operation-Level Shared-Mode Predicate Locks w.r.t Read Operations, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.
						Read Skew / Inconsistent Reads: A Transaction Reads differing computed Data that is based on some other previously Read Data, which became stale due to updates from another concurrently executed and completed Transaction.

						Lost Updates: A Transaction causes a prior committed update of a concurrently executed Transaction to a Record to be lost, because it based its own update on stale Data from the same Record via a prior Read. Solvable by the same methods used to solve Fuzzy Reads / Non-Repeatable Reads.
						Write Skew: A Transaction Writes differing computed Data that is based on some other previously Read Data, which became stale due to updates from another concurrently executed and completed Transaction.

					Phantoms: Incorrect behaviour stemming from stale Read Data that would not include new relevant additions / deletions whilst the Transaction was paused:
						Reads: A Transaction re-executes a Read and finds a different set of Records vs. the set of Records from executing the Read before due to committed additions / deletions from another concurrently executed Transaction. Solvable via Transaction-Level Shared-Mode Record and Predicate Locks w.r.t Read Operations, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

						Writes: Transactions each Read some Records to make a decision on whether to add / delete new Records, which can occur incorrectly and violate some constraint. Solvable via Conflict Materialization, which creates Dummy Records to prefill a Query Space so that Locks can be placed when Dummy Records are to be Written to.

				Isolation Levels: 
					Note: The Database Engine may comply by implementing a higher Isolation Level than what is required by Read Committed - check if there are other Isolation Levels offered by the Database Engine.

					Read Uncommitted: A Write Operation within a Transaction attempts to obtain an Transaction-Level Exclusive-Mode Record Lock.
						Features:
							Prevents Dirty Writes.

					Read Committed: Read Uncommitted + Read Operations within a Transaction attempts to obtain Record-Level Shared-Mode Record Locks.
						Features:
							Prevents Dirty Writes + Dirty Reads.

						Scenarios:
							Useful in cases where the Query only needs approximate / best-effort Data.

					Snapshot Isolation: Each Write to a Record results in a separate copy stored alongside a Monotonically Increasing Transaction ID Sequence. Reads can only view copies of Records that are lower than the Transaction ID it is attached to, and Writes can be aborted and retried when there exists a copy with a higher Transaction ID.
						Features:
							Prevents all Concurrency Bugs except Phantoms and Write Skews.
							Reads do not block Writes.
							Consumes more resources than Locking-based approaches.

					Repeatable Reads: The Transaction attempts to obtain Transaction-Level Record Locks.
						Features:
							Prevents all Concurrency Bugs except Phantoms and Write Skews.

					Serializable Snapshot Isolation: New. Snapshot Isolation, but track for cases where non Serializable Execution could occur, and abort one of the participating Transactions (e.g. track all Reads for a Record, and abort all tracked Reads when a Write is made to the Record).
						Features:
							Prevents all Concurrency Bugs, but may unnecessarily abort certain Transactions.
							Reads do not block Writes.
							Faster than traditional Two Phase Locking.
							Consumes more resources than Locking-based approaches.

					Serializable / Two Phase Locking: Widely used - all necessary Locks are acquired / are upgraded for a Transaction (Growth Phase) before Locks are released / are downgraded (Shrink Phase, which usually occurs when a Transaction completes / is rolled back).
						Features:
							Prevents all Concurrency Bugs, but frequent Deadlocks and unnecessary slowdown for Transactions that may not have actually caused Concurrency Bugs - the Database must detect when Deadlocks occur, and abort one or more Transactions to release the Deadlock, then retry the aborted Transactions.
								Example: Multiple Transactions holding the Shared Lock for an Object, and these Transactions want to Write to the Object (Write-Write Conflict).

					Actual Serial Execution: Implement all Queries on a Single Thread / Single CPU Core.
						Preconditions:
							In-Memory Database (as IO takes too long for Disk)
							Stored Procedure (store Transaction ahead of time instead of sending over SQL Operations over Network, as that takes too long)
							Transactions only affect one Partition (one CPU with Network Delay is very slow)

						Pros:
							Prevents all Concurrency Bugs.
							Easy to implement.
							Non-Analytics Transactions are usually short.

						Cons:
							Throughput limited to a single CPU Core.
								Does not work well with Distributed Scenarios due to requirement of Network Operations.

							Stored Procedures:
								Need to know ahead of time the Query to execute.
								Hard to Version Control.
								May not be compatable with many popular Programming Languages used for the Backend.

			Durability:			Transactions that have been committed will remain committed, even during System Failure (Power Outage / Crash).

		Database Objectives:
			Persistent Data Storage: Data should not be lost when the Database shuts down.

			Available Writes
			Available Reads
				Data should be stored contiguously on Disk to minimize the amount of Disk Lookups required (known to be expensive as it involves mechanical movement of the Storage Disks at the Hardware level).

				CAP Theorem: Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3).
					Configurations:
						For a Distributed Database, Partition Tolerance is needed as Networks are unreliable, so either Consistency or Availability is compromised.
							If Consistency is prioritised, that means the Database must for e.g. prevent Write Requests from succeeding until all Replicas can be reached again.
							If Availability is prioritised, that means that the Data between Replicas can go out of sync during certain scenarios.

						For a Centralized Database need not have Partitioning Tolerance, so both Consistency and Availability is achievable.

					Replication: Primarily to ensure some degree of Data Availability and Partitioning Tolerance.
						Consistency Models: In ascending order of resultant Consistency / descending order of resultant Availability:
							Weak Consistency: Each Read on an Object is only guaranteed to return the result of applying a subset of Writes to that Object thus far (e.g. Writes 2 to 3, Writes 1 to 4, Writes 1 & 3 to 5). This means that each Read has a chance to return a version of the Object that has never actually existed.

							Eventual Consistency: Weak Consistency in the short term, but will reach Strong Consistency in the long term.

							Consistent Prefix: Each Read on an Object is guaranteed to return the result of applying an ordered sequence of Writes on an Object thus far, starting from the first Write (e.g. Writes 1 to 3, Writes 1 to 2, Writes 1 to 5). This means that each Read will return a version of the Object that has actually existed at some point in time.

							Session Consistency: Data Abnormalities should not occur for Reads and Writes during a limited period of time.
								Monotonic Reads: Each Read in a sequence of Reads on an Object is guaranteed to return the result of applying at least the same subset of Writes on the Object thus far as the previous Read (e.g. Writes 3 to 4, Writes 3 to 4, Writes 1 & 3 to 4, Writes 1 to 5).

								Read My Write: The result of all Writes to an Object done by a Client is guaranteed to be observable in the Client's subsequent Reads done on the same Object (i.e. Strong Consistency only guaranteed for the Client).

							Bounded Staleness: Each Read on an Object is guaranteed to give the result of applying at least x Writes to that Object thus far (e.g. at least Writes 1 to 3, if x = 3 => Writes 1 to 3 & 5).

							Strong Consistency: A Read is guaranteed to return the result of applying all Writes on an Object thus far. Performance-intensive; depending on use case, it may be possible to use the weaker Consistency Models for the same effect. Note that this is technically Bounded Staleness with x = n.
								Linearizability: Once a Write is committed to an Object, all further Reads on that Object should observe the effect of the Write (i.e. it appears that only a single copy of the Object exists). In the context of a Distributed Database, this means that the Write must also be propagated in such a manner where it can be considered to have occurred instantaneously across all Nodes.
									Total Ordering: Every Operation that has been done to produce the current version of a stored Object on a Database should have a "happens-before" relationship with another Operation (no Operations should be observed to have executed concurrently, as is the case with Partial Ordering), such that the group of Operations must have some expressible order. In the context of Distributed Databases implementing Strong Consistency, the ordering must be the same across all Replicas.
										Lamport Timestamp: A Tuple containing a Counter and some ID of the Node that the Object is stored in.
											Each time a Process attempts to access the Object in a Node of a Distributed Database in some Operation, the Process must update the value of the Counter within the Lamport Timestamp in the Node to the maximum of what the Process has seen thus far, plus 1, as well as track this new value itself for later interactions with the Object.

											The Operation Order can be retroactively traced by sorting the Timestamps associated with each Operation by the Counter. If two Timestamps have the same Counter, it means the two Operations executed without knowledge of the other, and an arbitrary tiebreaker based on the ID should be used to order the Timestamps.

										Total Order Broadcast: A Protocol to ensure that every Replica in the Replica Set receives all Operations in the same order.
											The Operation Order can be traced in real-time.

						Types:
							Synchronous: Writes not considered successful until all Instances in the Cluster completes the Write Transaction. Strong Consistency, but slower Speed.

							Asynchronous: Writes considered successful when the Instance that was assigned the Write Transaction completes it. Faster Speed, but Clients may get stale / inconsistent Reads due to Eventual Consistency.
								Server / Database should simply return the written Data back to the Client that triggered the Write upon Success, or Read only from Leaders for editable Data (Read My Write Consistency).
								Each Client should only refer to one Instance for updates throughout the operation (Consistent-Prefix + Monotonic Reads Consistency), as different Replicas synchronize with a particular Write at different Speeds (Back-in-Time syndrome).
								Writes with Casual Relationships should ideally happen on the same Disk Partition (Consistent Prefix Reads), otherwise they might not make sense when they are Read (different Read Speeds for different Disk Partitions).

						Strategies:
							Single-Leader: All Writes to one Leader. Reads can come from any Replica (Follower / Leader) in the Cluster.
								Characteristics:
									Leader sends list of updates to other Followers via a Replication Log.
										Implementation:
											SQL Statements (for Relational Databases): Not ideal as SQL Statements can be non-deterministic (e.g. reference Current Time, which can be different per Replica).
											Internal Append-only Log: Describes which Bytes were changed. Not ideal as different Instances might have the affected Data in different Locations.
											Logical Log: Describes which Data was modified and how. Better future-proofing if underlying Database Engine changes.

									Adding a Follower: Initialize the Follower with a consistent snapshot of the Data in the Leader (associated with some position in the Replication Log), and then update to the latest State via the Replication Log.

									Pros:
										Single Source of Truth => no Conflicts to resolve.
									
									Cons:
										All Writes going to a single Instance, so Write Throughput is lower.
											Can consider Sharding, and having different Leaders for each subset of the Data.

								Scenarios:
									Follower Failure: On reboot, fetch all changes from the Leader, and implementing the changes starting from its last position in the Replication Log.

									Leader Failure / Failover: Determine a new Leader based on some Consensus / most up-to-date Follower, and configure downstream Dependencies to send Writes to this new Leader + configure all other Follower to get the changes from the new Leader.
										Problems:
											Unreplicated / Partially Replicated Writes of old Leader to Followers.
											Accidental Failovers from temporary Network Congestion.
											Split Brain: Old Leader associates itself to still be a Leader upon recovery and accepts Writes, leading to an inconsistent Data State across the Cluster.

							Multi-Leader: Writes to one of many possible Leaders (possibly across Data Centers) - Leaders send each other Writes through some existing topology. Reads can come from any Replica (Follower / Leader) in the Cluster. 
								Characteristics:
									Leaders can send each other Writes via several Topologies:
										Circular: A ring consisting of Leaders - each Leader passes Writes down to the next Leader after it in the ring. Not very Fault Tolerant.
										Star: Akin to a starfish - every Leader is connected to some central Node. Single Point of Failure.
										All-to-All (Fully Connected) - Each Leader is connected to every other Leader.

										Chain Replication?

									Pros:
										A Leader in each Data Center makes it easier to manage globally available Services.
										Write Throughput not limited by a single Replica.

									Cons:
										Write Conflicts possible between multiple Leaders.

								Scenarios:
									Detecting Concurrent Writes: Writes are considered Concurrent if each Source of the Write did not know of the Write from the others (compare with Causal Writes), NOT about time at which the Writes took place (i.e. it's all about keeping track of what the Source sees before making the Write).
										Version Vectors: Keep track of the number of Writes that each participant (Client / Replica) *thinks* all Replicas have made for every Object receiving Writes. 
											Each Replica persists their view of the Version Vector, and passes it to the Client, which it then passes back to the Replica during a Write for tracking, and update the Version Vector before passing it back to the Client again.

											Between two Version Vectors:
												If all Elements of one Version Vector is equal or greater than all corresponding Elements of another Version Vector, then it means that the Write for the first Version Vector happened after the other (Writes have the Happen-Before Relationship).
												If one Version Vector has Elements that are either smaller or greater than the corresponding Elements in the other Version Vector, then it means Concurrent Writes have occurred.

											Version Vectors are merged when the Consistency Model is triggered by taking the maximum of each Element between each Version Vector tracked per Replica.

									Conflict Resolution Strategies:
										Conflict Avoidance: Have all Writes for an Object go to one Leader. Not good for flexibility in Configuration (e.g. during Leader failure, or Horizontal Scaling).

										Last Write Wins: Each Write is given a Clock Timestamp, with the latest one being kept. Easy to implement, but Writes will be lost (important for use-cases requiring Version Histories) + possibility of Clock Skew between different Servers.

										On Read: Store all conflicting Writes for an Object. During a Read for the Object, resolve them and store the resolved value of the Object.

										On Write: Database relies on Application-defined Commutative (ordering doesn't matter) and Associative (which binary combinations don't matter after all are processed) Merging Logic to resolve Write Conflicts (Conflict-Free Replicated Data Types e.g. Distributed Counters, Decrementors, Distributed Sets, Ordered Lists).

							Leaderless: Writes to / Reads from all Replicas in parallel, and waits for a certain number of Replicas to return successfully (i.e. Quorum) before deeming operation a success.
								Characteristics:
									Quorum:
										Let W be the number of Replicas that need to succeed for a Client to consider their Write to be successful.
										Let R be the number of Replicas that need to succeed for a Client to consider their Read to be successful.
										Let N be the total number of Replicas in the Cluster. Typically N should be an odd number.

										Pseudo Strong Consistency Quorum: If W + R > N where N is the total number of Replicas, then at least one Replica will have the most updated Object.
											Typically W = R = (N+1)/2, then each Read / Write can tolerate up to (N-1)/2 Replicas failing per Operation.

											Imperfect:
												Allows for inconsistent Data States (i.e. Client considers Write failed as w < W, but w Replicas now have a potentially unwanted Write).
												Replica restoration after failure may take Data from another Replica that's more stale.
												During Sloppy Quorums, the Pseudo Strong Consistency Property no longer holds.

									Bookeeping Operations necessary in order to maintain Consistency between Replicas:
										Read Repair: On a Read, update those Replicas that has outdated Data with the Object with the highest Version Number (Eventual Consistency).

										Anti-Entropy: Background Process that occasionally looks at all Replicas to check for inconsistent versions for each Object's Version Number (Eventual Consistency / Bounded Staleness).
									
									Pros:
										Works well with Cross Data Center configurations by having W be small enough such that all Writes can go to one Data Center.
										Replication Strategy with the best flexibility for adjusting Consistency and Availabilty.

									Cons:
										Inconsistent Data still possible, even with a proper Quorum.
										Write Conflicts possible between multiple Replicas.
										Reads are slower + possibility of needing to do Read Repair.

								Scenarios:
									Conflict Resolution: See Multi-Leader Replication Section.

									Sloppy Quorums: Occurs when Replicas are temporarily added after the initial set of Replicas have all failed to handle further Writes - need to bring Written Data over to the initial set of Replicas once they have recovered (Hinted Handover).

				Optimization Techniques:
					Append-Only: New Records / Updated-versions of Past Records are simply appended to the end of the Collection without concern for Duplication. Lookups can be optimized by traversing from the end of the Collection.

					Indexes: Improves speed of Lookups, but imposes additional operations during Insertions / Deletions, and adds to the Disk Space required by the Collection.
						Background:
							The benefit of the Index is that it is able to reduce the number of Disk Block accesses maximally required to find the actual Disk Block that stores the complete Record, because only the necessary subset of the Record's Data is stored in each Disk Block used by the Index (thus each Disk Block can store more Block Addresses).
								The optimization is more effective when Records are large => Less Records per Disk Block => More Disk Block Lookups without an Index.

							The Data within the Index is usually stored in sorted order, but the actual Records in the Collection on Disk may not be stored in sorted order.

						Operations: For a given Key k (i.e. Fields) to return Value(s) v (i.e. Full Record location on Disk):
							insert(k, v)
							delete(k)
							[v1, v2, ... ] = query(k)
							[v1, v2, ... ] = range(k1, k2)

						Types:
							Hashmaps: Index Keys map to Memory Addresses (Byte Offsets) in Disk.
								Pros:
									Compatable with the Append-Only method of adding new Records to the Collection.
									Allows access to Records in Constant Time.

								Cons: 
									Entire Hashmap has to be In-Memory for it to be effective - useful for only small Datasets.
									Not good for Range Queries, as Data within the Index is unsorted.

							Log Structured Merge Trees + Immutable SS Tables: 
								Approach:
									Full Records realized from Database Writes are first placed in an In-Memory Balanced Binary Search Tree, with the Key used in the BST being the Index Key(s).
										To increase persistence, keep a Log on Database Writes that can be used to restore the In-Memory BBST after a Database Outage.

									When Tree gets too large, a Sorted SS Table is generated via BST In-Order Traversal and is transferred to Disk.
										The Tree gets emptied each time a SS Table is generated.

									There can be duplicate Keys between multiple Tables (and the Tree itself) - simply prioritise looking for the Key in the Tree, followed by the youngest SS Table to the oldest.
										Sparse Hashmaps with Memory Addresses + Binary Search on Table Keys can be additionally used to optimize lookups within an SS Table.
										Bloom Filters can be additionally used to optimize lookups between SS Tables.

										Tables can be combined in the same approach that Merge Sort uses to combine 2 Sorted Lists - Linear Time Background Operation.
											Prioritise the Key-Value Pair that is in the younger SS Table over the older SS Table.

								Pros:
									Higher Write Throughput due to Records being placed In-Memory at first.
									Uses the Append-Only method of adding new Records to the Collection.
									Good for Range Queries (that only use the Keys in the Index), as the Data in the Index is internally Sorted.

								Cons:
									Relatively Slow Reads for Keys that do not exist in the Collection.
									Merging Process for SS Tables can take up Background Resources.

							B Trees / B+ Trees: An extension of a Binary Search Tree - m-way Search Tree, but with constraints on how new Elements are inserted to prevent degenerate Trees.
								Approach:
									Each Node in the Tree is stored in one Disk Block.

									In the Non-Leaf Nodes, Keys of the Records used in the Index are stored alongside the pointers to next Node / Disk Block.
										For very large Collections, a Multi-Level Index may be used to avoid sequential Disk Block lookups in the original Index.
											At Higher Levels, the Keys stored in a Node / Disk Block becomes more sparse:
												Level 1: Keys 1 2 3, 4 5 6, 7 8 9, 10 11 12, 13 14 15, ...
												Level 2: Keys 1 4 7, 10 13 16, ...
												Level 3: Keys 1 10 19, ...

									Complete Records are only stored at the Leaf Nodes.

								Behaviour:
									Time Complexities:
										- Search / Update: O(logn)
										- Insertion: O(logn)
										- Deletion: O(logn)
									
									B-Tree Properties: 
										- Each Node can now store up to m-1 Elements, and m references to Children Nodes (m Degree / Order).
											- References to Child Nodes can be visualized as being stored 'between' the Elements.
											- Element Order in each Node is important.
										
										- Self-Balancing Properties:
											- All Non-Root Nodes with Children should have at least ceil(m/2) Child Nodes.
												- A Child Node can only be created if this property is fulfilled for the given Non-Leaf Node.

											- All Leaf Nodes should have at least ceil(m/2) - 1 Elements.

											- All Leaf Nodes (Nodes with no Children) should be at the same Level.
												- (Proactive) Insertion: Advantageous of not visiting a Node twice.
													- Bottom-Up: Tree grows upwards ; actual Element Insertion should only occur at a Leaf Node.
													- Traverse down from the Root Node to a Leaf Node.
														- If a given Node during Traversal is full, split the Node into 2 Child Nodes, and take the median Element from the original Node and set it to a Parent Node.
															- Left Bias: Resultant Left Child will have more Elements.
															- Right Bias: Resultant Right Child will have more Elements.

														- Continue Traversal from one of the Child Nodes.
												
												- (Proactive) Deletion: 
													- Traverse down from the Root Node.
														- If Element is in Non-Leaf Node:
															- If the Child Node that precedes / succeeds the Element has at least ceil(m/2)-1 Elements:
																- Obtain the Predecessor / Successor of the Element, depending on which Child Node has enough Elements.
																- Replace Element with Predecessor / Successor Element.
																- Delete Predecessor / Successor - guaranteed to be in Leaf Node.
																
															- Else: Both Child Nodes has less than ceil(m/2)-1 Elements:
																- Merge Both Child Nodes together, set Element to be Median Key of Child Node.
																- Traverse to new Child Node, and recursively delete Key from Child Node.

																- Note: Parent may have less than ceil(m/2)-1 Elements as a result of this operation, but this will be resolved in subsequent Deletes.

														- Else: Element is not in Current Non-Leaf Node.
															- Determine next Child Node to traverse to.

															- If Child Node has less than the minimum ceil(m/2)-1 Elements:
																- If an immediate Sibling Node has at least ceil(m/2) Elements, execute a left or right Rotation on one of the neighbouring Parent Elements.
																	- Send the neighbouring Parent Element down to the Child Node.
																	- Set the Predecessor or Successor Element of the Sibling Node in the Parent Element's position.
																- If both immediate Sibling Nodes have less than ceil(m/2) Elements, merge one of the Siblings together with the Child Node, with one the Parent Element becoming the Median Element of the Child Node.

															- Continue Traversal.
															- If Traversal has reached a Leaf Node, delete Element from Leaf Node if it exists.

										- A subset of the Tree can be stored in Cache, allowing for faster Query access.

									B+ Trees Properties:
										- Does not need to traverse the whole Tree for Sequential Access:
											- All Non-Leaf Nodes have their Elements in the Leaf Nodes as well.
											- All Leaf Nodes will be connected together like a Linked List from left to right.

								Pros:
									Higher Read Throughput - Sub-Linear Time Tree Traversal.
									Good for Range Queries (that only use the Keys in the Index), as the Data in the Index is internally Sorted.

								Cons:
									Relatively Slow Writes, as multiple Disk Blocks are accessed after each Write to update the Index.
	
							B-Epsilon / Fractal Tree Index: An extension of B Trees.
								Behaviour:
									- Each Non-Leaf Node now has a large buffer that accumulate Writes.
										- The buffer is part of the Disk Block that the Node resides in, and implemented as a BBST or something that allows faster searches.
											- Each Write is linked to a Timestamp as well to maintain correct Operation Order.

										- The Root Node receives the Writes first.
											- Deletions are "negative" Writes implemented with Tombstone Messages.

									- When a Node's buffer is full, it will flush some of the accumulated Writes down to its Child Nodes.
										- More than one Write is transferred from Parent Node to Child Node at a time, which saves on expensive Disk Block accesses.
											- Compared to regular B-Trees where Writes would each individually require 2 Disk Block accesses vs transferring all Writes at one go (using Memory as an intermediary).

											- Writes are only flushed if the Child has enough pending Writes to offset the cost of re-writing the Parent and Child Nodes.

										- If all Writes are to go to the same Leaf Node, optimizations should be made to immediately flush the Writes (and any other in the Path) directly to the Leaf Node, instead of adding and removing the Writes from each intermediate Node from the Root to the Leaf.

									- A Write is fully processed when it reaches a Leaf Node.

								Pros:
									Higher Write Throughput compared to B-Trees.
										Asymptotically same Read Performance, but is actually up to 2x more Disk Block Accesses.

										Batching Writes allow for Larger Nodes (multiples of Disk Block size) vs. regular B Tree Nodes, in terms of Write Cost.

								Cons:
									Equivalent performance to B-Trees when checking for unique Index Keys.

					Partitioning: Splitting up a large Collection into smaller non-overlapping Partitions such that each Partition can be stored and maintained in separate Nodes. Each Node can contain more than one Partition.
						Partitioning is usually only considered when a Collection becomes too big to be stored in one Node, as it adds a lot of underlying complexity to the System.
							Gossip Protocol (Database Nodes communicate between themselves)
							Layer to track which parts of the Dataset is stored in which Partition / Node.

						Often used alongside Replication (i.e. Nodes have separate Replica Sets).

						Objectives:
							Similarly sized Partitions (not necessarily equal in size as there might be certain Partitions that get more attention normally than others).
							Overall Traffic distributed equally per Node.

							Nodes become "Hotspots" if these are not met for any Node in the Partition Set.

						Configurations:
							Fixed Number of Partitions:
								Too Low:
									Each Partition will get too big, and Application can't scale.
									Rebalancing will take a longer time.

								Too High:
									Larger Overhead on Disk for each Node w.r.t keeping track of all Ranges and Disk Locations for each Partition.

							Fixed Number of Partitions per Node:
								Each Node has a certain number of Partitions that grow proportionally to the Dataset.
									Upon adding a new Node it will split existing Partitions from each Node, and take a piece of each split for itself.

							Dynamic Partitioning: Some Databases are able to adjust Partitions automatically to reduce Hotspots as Data Access Patterns change over time.
								Splitting and Merging of Partitions.

						Methods:
							Note: Don't use a Modulo on the number of Partitions to determine which Key goes into which Partition, as that can change in the future and will require Records to be reshuffled around Partitions / Nodes.

							Ranges: Each Partition involves a Range which is a Contiguous Ordered Sequence of Values. Ranges can be of different sizes to avoid Hotspots.
								Of Keys: Use Record Keys as they are to determine which Partition / Range they belong to (e.g. Alphabetical Ranges).
									Pros:
										Simple to understand.
										Compatible with Range Queries.

									Cons:
										Need to actually ensure Ranges meet Objectives (e.g. Timestamp Ranges can lead to Hotspots).

								Of Hashes of Keys: Hash Record Keys before determining which Partition / Range they belong to.
									Pros:
										Assuming a good Hash Function, Keys are evenly distributed.

									Cons:
										Not ideal for Range Queries.
										Hotspots can still occur if a few Record Keys are always requested for.

						Rebalancing: Should attempt to keep a majority of the Records in the same Node to avoid high Bandwidth costs.
							During the process the old Node will take on the Reads and Writes for the Data involved in the migration to the new Node.

						Compatability with other Database Mechanisms:
							Secondary Indexes:
								Local Indexes: Hold a Secondary Index that only tracks the Data on the Partition (i.e. each Partition holds all Index Entries, but each Index Entry only tracks the Records on the current Partition).
									Pros:
										Fast for Writing since all Writes are within the Partition.

									Cons:
										Slow on Read, as all Local Indexes in all Partitions need to be aggregated / processed to determine which Partition has the Record.

								Global Indexes: Partition the Secondary Index as well (i.e. each Partition holds a subset of the Index Entries, but each Index Entry tracks all Records (regardless whether or not they are within the same Partition) in the Dataset that belong to that Index Entry).
									Pros:
										Fast for Read since all related Data for the Index Entry is stored in one Partition.

									Cons:
										Slow on Write, as Writes now potentially need to go to multiple Partitions (one for the Record, and one / two to update the Index Entry).
										Consensus or Transaction Mechanism required in case Write to Record succeeds and Update to Index Entry (on a different Partition) fails.

				Consensus:
					Two-Phase Commit: Consensus Protocol that allows for Atomic Commits (i.e. Operations within a Transaction either all succeed or fail on all involved Nodes).
						Setup:
							A Coordinator Process / Node is required.
							Every Transaction should have a monotonically increasing Transaction ID.

						Phases:
							Request Phase:
								The Coordinator sends a Query to Commit Message once all Nodes have received their set of Operations.
								The Nodes executes the Operations up to the Commit Point.
									They additionally update their Undo and Redo Logs, in case one of their siblings cannot continue.

								The Nodes replies to the Coordinator an "Ack" or "Abort" Message, depending on successful or failed execution of the Operations respectively.

							Commit Phase:
								Success: 
									Once the Coordinator receives "Ack" from all Nodes involved in the Transaction, it will send a "Commit" Message to the Nodes.
									The Nodes will then Commit the changes from the executed Operation, and release all Locks and Resources held during the Transaction.
									The Nodes will then send an "Ack" back to the Coordinator, which will then mark the Transaction as completed once it has received the response from all Nodes.

								Failure:
									Once the Coordinator receives an "Abort" from any of the Nodes involved in the Transaction, it will send an "Abort" Message to the Nodes.
									The Nodes will then undo the Transaction using the Undo Log, and release all Locks and Resources held during the Transaction.
									The Nodes will then send an "Ack" back to the Coordinator, which will then mark the Transaction as aborted once it has received the response from all Nodes.

						Pros:
							Allows for Atomic Commits.
							Simple Consensus Mechanism.

						Cons:
							Not Fault Tolerant as the Coordinator is a Single Point of Failure.
								Alleviated by allowing for Relicated Coordinators, though depending on configuration this may slow down performance.

							If both the Coordinator and a Node fails, it will not be possible to determine if the Commit Phase was initiated with what state, as the Node that failed could have received the Commit Phase Message.
								Alleviated by using Three-Phase Commit, which introduces an additional Prepare-to-Commit Phase.
							
							Blocking: Further Operations cannot proceed until the Coordinator / all Nodes have succeeded in executing their Operations.

					Raft: Consensus Protocol used in Single Leader Replication to automatically elect a new Leader during Failover and build Replication Logs (achieving Total Order Broadcast).
						Setup:
							Each Replica maintains a Term (Fencing Token).
							Each Replica maintains its own Log of Database Operations performed (a.k.a Replication Log), and each Operation is associated with a Term.
						
						Normal Behaviour:
							Each Replica is either in a Follower, Candidate or Leader State:
								Follower:
									A Replica that awaits to receive compatible updates in the form of Heartbeat Messages from a Leader.

									All Replicas are Followers upon startup.

									The moment any Replica receives a Message from another Replica that has a higher Term, the former must become a Follower and update its own Term to match.

								Candidate:
									A Follower upgrades itself to a Candidate State when it has not received a Heartbeat Message with the same or greater Term Number / a Vote Request with a greater Term Number after a certain amount of time (randomized for each Follower to avoid scenarios where it becomes impossible to form a Quorum).
										It will increase its Term by 1, and send Election Requests to the rest of the Followers whilst voting for itself.
											The content of an Election Request contain the index and Term of the last Operation that the Follower has commited thus far.

										Scenarios:
											If a Replica has already cast a vote for the new Candidate this Term (either for itself as a Candidate, or another Candidate), it will not vote for other Candidates.

											If a Replica receives an Election Request from a Candidate with a higher Term, in addition to becoming a Follower and updating its own Term to match:
												If the Election Request comes from a Candidate with a more updated Log, the Follower will cast the vote for the Candidate.

											Else it will not vote for the Candidate that sent the Election Request.

										Outcomes:
											If a Quorum is reached in its Favour, it will immediately send the next Heartbeat Message to all other Followers, self-declaring that it is the new Leader.

											If a Quorum is not reached in its Favour, it will remain as a Candidate till it either receives a Heartbeat Message from a new Leader or its Election Timeout expires, afterwhich it will re-elect itself as a Candidate, repeating the Election Process.

								Leader:
									A Candidate upgrades itself to a Leader once it has received votes in its favour from a Quorum of the other Replicas.

									The Leader must attempt to track the Prefix and Suffix for each Follower:
										Prefix: An estimate on the Operations that the Follower has successfully received.
										Suffix: An estimate on the Operations that the Leader has sent to the Follower, but has not been successfully received.

									A Leader would propagate updates to its Followers one at a time, even if there are multiple in its queue:
										The Heartbeat Message from the Leader containing the update has the following information:
											The current Term of the Leader.

											The index and Term of the last Operation in the Prefix of the Follower.
										
											The first Operation in the Suffix of the Follower. 

											The estimated number of Commits that the Follower has made.

										The Leader executes the Operation but does not commit it, instead propagating the Operation to its Followers first.
											Outcomes:
												If the Follower's Term of the last Operation in the Prefix matches the one in the Heartbeat Message, the update will be successful.
													Any Operation that exists in a majority of the Replicas will take precedence, and eventually be committed, otherwise no guarantee.
												
												Else, the Leader will have to keep reducing the Prefix and increasing the Suffix until the update succeeds, or become a Follower itself if the Follower has a higher Term.

										The Followers execute the Operation but do not commit it, responding back to the Leader upon success.

										The Leader waits to receive successful responses from a Quorum, and proceeds to commit the Operation.
										The Leader then sends the commit request to its Followers, making them commit the Operation.

										The entire process repeats for each subsequent update.
							
						Special Scenarios:
							There can be more than one Candidate during an Election.
							A Leader will become a Follower if another Follower that did not manage to receive Heartbeat Messages becomes a Candidate and increments its Term.

							If the Replica Set suffered a Network Partition such that the Replica Set was split into two groups and the existing Leader is in a group that is smaller than the other, then there will eventually be two Leaders. However, the old Leader won't be able to commit Operations because it won't have access to a Quorum.

						Pros:
							Fault Tolerant.

						Cons:
							Not usable for achieving Consensus in Multi-Leader / Partitioned Setups; strictly for Single Leader Replication.

							Blocking: Further Operations cannot be committed until the oldest uncommitted Operation succeeds in being committed.

		Database Types:
			In-Memory Database: 	A Database which stores Data in Main Memory, which is much faster than Disk Storage as I/O is skipped. Suitable for Operations in which Response Time is critical.
				Pros:


				Cons:


				Examples:
					Redis
					Memcache

			Relational Database: 
				Characteristics:
					Consists of Tables storing Rows of Structured Data, with Rows on one Table having a Relation to Rows in another if both share common Key(s).
					Reduction of Data Duplication via Normalization and JOINing.
					Each Table has one Schema, which eases Data Encoding.

					All Reads and Writes go to Disk.

					Uses B-Trees for their Indexes.

					Hide Concurrency Bugs and Failures via Transactions.
					2-Phase Locking for supporting Transaction Concurrency.

				Pros:
					ACID-strengthened Transactions.

					Relational Structure allows minimization of Data Duplication across Tables.

				Cons:
					Set Schema does not allow Data to flexibly adapt to changing Application needs easily.

					Scalability:
						Challenging to Horizontally Partition since RDBMS has to ensure that all parts of the (Distributed) Transaction succeed or fail as a whole, which can involve multiple Nodes and even more Network Calls.
							JOINs would require aggregation across Nodes.
							Distributed Transactions required for Writes.

						B-Trees are slow.
						2-Phase Locking is slow.

					Row-Based Data Structure not optimal for Data Compression.

					Generally limited to Single-Leader Replication.

				Related Content:
					SQL
					NewSQL:		Seeks to combine the strengths of Relational Databases with the strengths of Document Databases.

					Data Warehouse: A Secondary Database that stores computed Data from a Production Database in a way that is more efficient for long running Analytical Queries.

				Examples:
					MySQL
					PostgresSQL

					NewSQL:
						VoltDB
						Google Spanner
							All Servers have an attached GPS Clock, which provides correct measurement of Event order.

					PrestoDB:	Facebook's Distributed SQL Query Engine.

			Non-Relational Database: Anything that is not storing Data in a Structured Table Format:
				Types:
					Document Database:
						Pros:
							Documents are self-containing; they group relevant Data together.
								Performance: Improved Data Locality due to self-containing nature of Documents, which results in more efficient Sequential Disk Lookups.
								Scalability: Easier to Horizontally Partition due to self-containing nature of Documents.

						Cons:
							Data Duplication.

							Not suited for scenarios where there are many Relations between Data, since Documents are supposed to be self-containing.

							Larger Network Calls as a result of sending over whole Documents with unneeded Information.

						Related Content:

						Examples:
							MongoDB
							DynamoDB:	Amazon's NoSQL Solution.
							Firestorm

					Graph Database: Stores Data in a Graph (i.e. Vertices and Edges). Queries involve traversing the Graph.
						Pros:
							Does well for Data with many-to-many Relationships.

						Cons:

						Related Content:

						Examples:
							Cassandra
								Uses Timestamps to measure ordering of Events; recommends keeping Servers updated with NTP as much as possible.

					Key-Value Database:
						Pros:

						Cons:

						Related Content:

						Examples:
							Redis: In-Memory based Database with Disk Persistence that is often used as a Caching Layer on top of other RDBMSes.

					Wide-Column Database:
						Pros:
						
						Cons:

						Related Content:

						Examples:

	Commands:
		
	
