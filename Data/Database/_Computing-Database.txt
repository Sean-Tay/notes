Database
	Abbreviations: 
		DBMS:					DataBase Management System.

		CMDB: 					Configuration Management Database.

	Description: 
		A Database is an Organized, Long-Term Collection of Data, generally Stored, Managed and Accessed via some System.
	
	Resources:
	
	Concepts:
		Data Types:
			Structured Data:	Data whose Elements are Directly Addressible (i.e. Tabular or Array-like Data).
			Semi-Structured Data: Data whose Elements may not be Directly Addressible, but still contain other forms of Organizational Properties (i.e. Tagged Data: XML).
			Unstructured Data:	Data which is currently observed to have no immediately helpful Pre-Defined Format (i.e. Binary Data: PDFs).

		Attribute / Key / Field: Refers to a characteristic of some type of Entity.
		Record:					A collection of Attributes pertaining to some instance of an Entity.
		Collection:				A group of Records.

		Write-Ahead Log: 		An Append-Only File that tracks Database Writes, usually updated before the actual Write itself happens as a way to ensure Persistence.

		Indexes:				Improves speed of Queries, but imposes additional operations during Insertions / Deletions, and adds to the Disk Space required by the Collection.
			Primary Index: 		Each Index Entry is unique (1:1 relation to the Full Records) as it includes the Primary Key of the Records.
			Secondary Index:	Index Entries do not include the Primary Key of the Records - each Index Entry may thus track more than one Record.

			Sparse Index:		Contains a mapping only for every n Records in the Collection. Typically used in Multi-Level Indexing (i.e further Indexing of a Dense or overly Large Index).
			Dense Index:		Contains a mapping for every Record in the Collection.

			Clustered Index: 	Each Index Entry stores the full Record / Collection Records are physically stored in sorted order.
			Covering Index: 	A partial Clustered Index; stores only a subset of the complete Record.

			Concatenating Index: An Index which concatenates the Keys inolved in the Index together, so that it need only use one Key. It is not able to handle Queries that does not at least use the first Key in the concatenation.
			Multi-Dimensional Index: An Index that is able to handle multiple Keys without needing to concatenate them together into one Key.
				Geospatial Index: An Index which specializes in handling queries associated with Global Coordinates.

			Fuzzy Index:		An Index which is able to handle Approximate Searches (i.e. Queries that don't use the exact Key Value as it was stored).

			Forward Index:		A Data Structure that maps the Record to the Record's Content.
			Inverted Index: 	An additional Data Structure (separate from the Collection) that allows for quick access of a Full Record via a defined subset of the Record's Content.

		Transaction:			A Unit of Work done / Measure of Change performed within a DBMS.
		Heterogenous Transaction: Transactions implemented using the generic XA API that would work for all Databases and setups.
		Internal Transaction: 	Transactions implemented using the specific Database Software's API that would be able to leverage on stronger characteristics of the Database being used.

		Operation:				A Statement within a Transaction.
		ACID:					A set of desirable properties to apply to a Transaction when interacting with a Database.

		Serializability: When the history of Operations being executed concurrently for one or more Objects is the same as one where the Operations are executed sequentially (i.e. no interleaving of Operations).
			Serialization Anomaly: The final Database State that is the result of successfully commiting a group of Transactions is inconsistent with any of all possible Database States arising from all possible orderings of running those Transactions sequentially.

			Strict Serializability: A combination of both Linearizability and Serializability. When the history of Operations being executed concurrently for one or more Objects is the same as one where the Operations are executed sequentially, and in the submitted order (i.e. no interleaving of Operations + correct order of Operations).

		Record-Level Lock: 		A Lock that lasts for as long as the interaction of one Record.
		Operation-Level Lock: 	A Lock that lasts for as long as one Operation within a Transaction.
		Transaction-Level Lock: A Lock that lasts until the completion or abortion of the enclosing Transaction.

		Shared-Mode Lock:		A Lock that allows concurrent Read Access but not Write Access. Obtainable when the involved subject has no Exclusive-Mode Lock active.
		Exclusive-Mode Lock: 	A Lock that allows for both Read and Write Access. Only one Transaction may hold this at any time, and only if the involved subject has no other existing Locks active.

		Record Lock: 			A Lock on a Record.
		Predicate Lock:			A Lock on a potential range of Records.

		Multiversion Concurrency Control: Generates a new version of an Object for each Write, and maintaining the last few relevant versions of the Object for Read Operations.

		CAP Theorem: 			Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3).
			Consistency:		Degree at which Data is the same throughout the Database.
				Consistency Model: Defines a contract in implementation to guarantee certain properties of Reads / Writes to Data in a Database. One or more can be implemented to achieve greater Consistency.
				
				Linearizability: An Operation (or a set of Operations) on a Object is seen as Linearizable when they can be considered completely executed and processed within an instantaneous point in time (with respect to other Operations).

			Availability:		Whether or not every Request for Reading / Writing Data is successful.
			Partitioning Tolerance: Enforces that the Database can still operate even if parts of it are unreachable in the Network.

		Replication:			Having Redundant Copies of the Data in a Database stored in multiple Nodes so that if one fails, dependent Applications can still use the other Nodes for Data Retrieval, achieving Reliability for the Distributed System. Also helps to reduce operational load per Instance.
		Replica:				A Node within a Replicated Database.

		Quorum:					A majority of Nodes that agree on something (requires total number of Nodes to be odd). It is impossible for two different Quorums to disagree on something, as the Quorums would have overlapping Nodes by definition.

		Database Partitioning:
			Horizontal Partitioning: Splitting a Collection into smaller non-overlapping Partitions and storing them in separate Nodes (i.e. Sharding). Each Node may contain more than one Partition.
			Vertical Partitioning: Different Record Attributes into Separate Collections (i.e. Normalization or simple Record Splitting).

			Hotspot:			A Node in a Database Cluster that receives a higher amount of Traffic and Load compared to others due to sub-optimal Partitioning.

	Details:
		ACID:					A set of Properties for Operations on a Database to guarantee so as to avoid Erroneous Data.
			Atomicity:			Transactions containing multiple Operations will either succeed as a whole, or not be processed at all.

			Consistency:		Transactions can only bring the Database from one correct State to another, respecting Constraints, Cascades and Triggers within affected Data. This is related to Data Consistency from the CAP Theorem, but not exactly the same.

			Isolation: 			The degree by which Transactions on a Database can assume that they would be unaffected by other concurrently executing Transactions, definable with the types of phenomena that can be observed during overlapping Transactions. The highest degree mandates that the end results of each Transaction can only be one of several, all of which arise from completely non-overlapping Transactions.
				Concurrency Bugs:
					Dirty Operations: Incorrect behaviour stemming from uncommitted Data.
						Writes: A Transaction overwrites committed Data from another concurrently executed Transaction (started later and completed earlier) with uncommitted Data. Uncommitted Data should be able to be subsequently rolled back, but the first Transaction will not be able to roll back without removing the committed Data. Solvable via Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

						Reads: A Transaction sees uncommitted Data from another concurrently executing Transaction. Uncommitted Data can be subsequently rolled back, which leaves the first Transaction with incorrect Data. Solvable via Operation-Level Shared-Mode Record Locks w.r.t each Read Operation, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

					Incorrect behaviour stemming from stale Read Data whose contents were updated whilst the Transaction was paused:
						Fuzzy Reads / Non-Repeatable Reads: A Transaction re-Reads previously read Data and sees that the content within the Read Records has changed. Solvable via Transaction-Level Shared-Mode Record Locks and Operation-Level Shared-Mode Predicate Locks w.r.t Read Operations, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.
						Read Skew / Inconsistent Reads: A Transaction Reads differing computed Data that is based on some other previously Read Data, which became stale due to updates from another concurrently executed and completed Transaction.

						Lost Updates: A Transaction causes a prior committed update of a concurrently executed Transaction to a Record to be lost, because it based its own update on stale Data from the same Record via a prior Read. Solvable by the same methods used to solve Fuzzy Reads / Non-Repeatable Reads.
						Write Skew: A Transaction Writes differing computed Data that is based on some other previously Read Data, which became stale due to updates from another concurrently executed and completed Transaction.

					Phantoms: Incorrect behaviour stemming from stale Read Data that would not include new relevant additions / deletions whilst the Transaction was paused:
						Reads: A Transaction re-executes a Read and finds a different set of Records vs. the set of Records from executing the Read before due to committed additions / deletions from another concurrently executed Transaction. Solvable via Transaction-Level Shared-Mode Record and Predicate Locks w.r.t Read Operations, and Transaction-Level Exclusive-Mode Record Locks w.r.t Write Operations.

						Writes: Transactions each Read some Records to make a decision on whether to add / delete new Records, which can occur incorrectly and violate some constraint. Solvable via Conflict Materialization, which creates Dummy Records to prefill a Query Space so that Locks can be placed when Dummy Records are to be Written to.

				Isolation Levels: 
					Note: The Database Engine may comply by implementing a higher Isolation Level than what is required by Read Committed - check if there are other Isolation Levels offered by the Database Engine.

					Read Uncommitted: A Write Operation within a Transaction attempts to obtain an Transaction-Level Exclusive-Mode Record Lock.
						Features:
							Prevents Dirty Writes.

					Read Committed: Read Uncommitted + Read Operations within a Transaction attempts to obtain Record-Level Shared-Mode Record Locks.
						Features:
							Prevents Dirty Writes + Dirty Reads.

						Scenarios:
							Useful in cases where the Query only needs approximate / best-effort Data.

					Snapshot Isolation: Each Write to a Record results in a separate copy stored alongside a Monotonically Increasing Transaction ID Sequence. Reads can only view copies of Records that are lower than the Transaction ID it is attached to, and Writes can be aborted and retried when there exists a copy with a higher Transaction ID.
						Features:
							Prevents all Concurrency Bugs except Phantoms and Write Skews.
							Reads do not block Writes.
							Consumes more resources than Locking-based approaches.

					Repeatable Reads: The Transaction attempts to obtain Transaction-Level Record Locks.
						Features:
							Prevents all Concurrency Bugs except Phantoms and Write Skews.

					Serializable Snapshot Isolation: New. Snapshot Isolation, but track for cases where non Serializable Execution could occur, and abort one of the participating Transactions (e.g. track all Reads for a Record, and abort all tracked Reads when a Write is made to the Record).
						Features:
							Prevents all Concurrency Bugs, but may unnecessarily abort certain Transactions.
							Reads do not block Writes.
							Faster than traditional Two Phase Locking.
							Consumes more resources than Locking-based approaches.

					Serializable / Two Phase Locking: Widely used - all necessary Locks are acquired / are upgraded for a Transaction (Growth Phase) before Locks are released / are downgraded (Shrink Phase, which usually occurs when a Transaction completes / is rolled back).
						Features:
							Prevents all Concurrency Bugs, but frequent Deadlocks and unnecessary slowdown for Transactions that may not have actually caused Concurrency Bugs - the Database must detect when Deadlocks occur, and abort one or more Transactions to release the Deadlock, then retry the aborted Transactions.
								Example: Multiple Transactions holding the Shared Lock for an Object, and these Transactions want to Write to the Object (Write-Write Conflict).

					Actual Serial Execution: Implement all Queries on a Single Thread / Single CPU Core.
						Preconditions:
							In-Memory Database (as IO takes too long for Disk)
							Stored Procedure (store Transaction ahead of time instead of sending over SQL Operations over Network, as that takes too long)
							Transactions only affect one Partition (one CPU with Network Delay is very slow)

						Pros:
							Prevents all Concurrency Bugs.
							Easy to implement.
							Non-Analytics Transactions are usually short.

						Cons:
							Throughput limited to a single CPU Core.
								Does not work well with Distributed Scenarios due to requirement of Network Operations.

							Stored Procedures:
								Need to know ahead of time the Query to execute.
								Hard to Version Control.
								May not be compatable with many popular Programming Languages used for the Backend.

			Durability:			Transactions that have been committed will remain committed, even during System Failure (Power Outage / Crash).

		Database Objectives:
			Persistent Data Storage: Data should not be lost when the Database shuts down.

			Available Writes
			Available Reads
				Data should be stored contiguously on Disk to minimize the amount of Disk Lookups required (known to be expensive as it involves mechanical movement of the Storage Disks at the Hardware level).
					Storage Engines: Responsible for the Storage and Retriebal of Data in a Database.
						Append-Only: New Records / Updated-versions of Past Records are simply appended to the end of Storage Files.
							Approach:
								Each Record can be stored as a line in the Storage File, alongside the Record's Primary Key.
									Lookups are done by traversing from the end of the File / newest Segment, taking the first instance of a Record that has the Primary Key desired.
										If a Tombstone was encountered instead, the Record should be considered as deleted.

								Whenever a File gets too big, it should be frozen into a Segment, while a new File should be used for new Writes. Multiple Segments can be compacted and merged together (i.e. take only the latest entry for each unique Primary Key) via a background process, as Segments are never modified once formed (new entries are always appended).
									If the latest entry is a Tombstone, skip putting the Key into the new Segment.

								Each Segment can have its own Index for quicker retrieval.

							Pros:
								Simple and Performant way of storing Records on Disk.

							Cons:
								During compacting and merging, all Record Keys in the involved Segments must be kept In Memory.
								Cannot handle Range Queries.
								The Index for each Segment needs to track every Record.

						LSM: An improvement over simple Append-Only Storage Engines - generate Segments of Records stored in sorted order of their Keys via LSM Tree Indexes.
							Approach:
								Full Records realized from Database Writes are first placed in an In-Memory Balanced Binary Search Tree, with the Key used in the BST being the Record's Primary Key / Index Keys.
									For persistence, maintain a Write-Ahead Log that can be used to restore the In-Memory BBST after a Database Outage.

								When Tree gets too large, a Sorted SS Table / Segment is generated via BST In-Order Traversal and is transferred to Disk.
									The Tree gets emptied each time a SS Table is generated.

								Tables can be combined in the same approach that Merge Sort uses to combine 2 Sorted Lists - Linear Time Background Operation.
									Prioritise the Key-Value Pair that is in the younger SS Table over the older SS Table.

							Pros:
								Performant way of storing Records on Disk.

								Compacting and merging no longer requires all Record Keys in involved Segnents to be kept in Memory, since Segments are now sorted and the Merge Sort Merging Algorithm can be used.
								Good for Range Queries (that use the Keys), as the Keys are stored in sorted order.

								Each Segment can now use a Sparse Index instead of a full Index, as Records within each Segment are now sorted.
									Bloom Filters can be additionally employed to check the existence of a given Key amongst all Segments, saving unnecessary Segment lookups.

				CAP Theorem: Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3).
					Configurations:
						For a Distributed Database, Partition Tolerance is needed as Networks are unreliable, so either Consistency or Availability is compromised.
							If Consistency is prioritised, that means the Database must for e.g. prevent Write Requests from succeeding until all Replicas can be reached again.
							If Availability is prioritised, that means that the Data between Replicas can go out of sync during certain scenarios.

						For a Centralized Database need not have Partitioning Tolerance, so both Consistency and Availability is achievable.

					Replication: Primarily to ensure some degree of Data Availability and Partitioning Tolerance.
						Consistency Models: In ascending order of resultant Consistency / descending order of resultant Availability:
							Weak Consistency: Each Read on an Object is only guaranteed to return the result of applying a subset of Writes to that Object thus far (e.g. Writes 2 to 3, Writes 1 to 4, Writes 1 & 3 to 5). This means that each Read has a chance to return a version of the Object that has never actually existed.

							Eventual Consistency: Weak Consistency in the short term, but will reach Strong Consistency in the long term.

							Consistent Prefix: Each Read on an Object is guaranteed to return the result of applying the correct sequence of Writes on an Object thus far (e.g. Writes 1 to 3, Writes 1 to 2, Writes 1 to 5). This means that each Read will return a version of the Object that has actually existed at some point in time.

							Session Consistency: Data Abnormalities should not occur for Reads and Writes during a limited period of time.
								Monotonic Reads: Each Read in a sequence of Reads on an Object is guaranteed to return the result of applying at least the same subset of Writes on the Object thus far as the previous Read (e.g. Writes 3 to 4, Writes 3 to 4, Writes 1 & 3 to 4, Writes 1 to 5).

								Read My Write: The result of all Writes to an Object done by a Client is guaranteed to be observable in the Client's subsequent Reads done on the same Object (i.e. Strong Consistency only guaranteed for the Client).

							Bounded Staleness: Each Read on an Object is guaranteed to give the result of applying at least x Writes to that Object thus far (e.g. at least Writes 1 to 3, if x = 3 => Writes 1 to 3 & 5).

							Strong Consistency: A Read is guaranteed to return the result of applying all Writes on an Object thus far. Performance-intensive; depending on use case, it may be possible to use the weaker Consistency Models for the same effect. Note that this is technically Bounded Staleness with x = n.
								Linearizability: Once a Write is committed to an Object, all further Reads on that Object should observe the effect of the Write (i.e. it appears that only a single copy of the Object exists). In the context of a Distributed Database, this means that the Write must also be propagated in such a manner where it can be considered to have occurred instantaneously across all Nodes.
									Total Ordering: Every Operation that has been done to produce the current version of a stored Object on a Database should have a "happens-before" relationship with another Operation (no Operations should be observed to have executed concurrently, as is the case with Partial Ordering), such that the group of Operations must have some expressible order. In the context of Distributed Databases implementing Strong Consistency, the ordering must be the same across all Replicas.
										Lamport Timestamp: A Tuple containing a Counter and some ID of the Node that the Object is stored in.
											Each time a Process attempts to access the Object in a Node of a Distributed Database in some Operation, the Process must update the value of the Counter within the Lamport Timestamp in the Node to the maximum of what the Process has seen thus far, plus 1, as well as track this new value itself for later interactions with the Object.

											The Operation Order can be retroactively traced by sorting the Timestamps associated with each Operation by the Counter. If two Timestamps have the same Counter, it means the two Operations executed without knowledge of the other, and an arbitrary tiebreaker based on the ID should be used to order the Timestamps.

										Total Order Broadcast: A Protocol to ensure that every Replica in the Replica Set receives all Operations in the same order.
											The Operation Order can be traced in real-time.

						Types:
							Synchronous: Writes are not considered successful until all Instances in the Cluster completes the Write Transaction. Strong Consistency, but slower Speed.
								If one Instance is not reachable, the Write will be delayed until it is reachable again.

							Asynchronous: Writes are considered successful when the Instance that was assigned the Write Transaction completes it. Faster Speed, but Clients may get stale / inconsistent Reads due to Eventual Consistency.
								Server / Database should simply return the written Data back to the Client that triggered the Write upon Success, or Read only from Leaders for editable Data (Read My Write Consistency).
								Each Client should only refer to one Instance for updates throughout the operation (Consistent-Prefix + Monotonic Reads Consistency), as different Replicas synchronize with a particular Write at different Speeds (Back-in-Time syndrome).
								Writes with Casual Relationships should ideally happen on the same Disk Partition (Consistent Prefix Reads), otherwise they might not make sense when they are Read (different Read Speeds for different Disk Partitions).

							Semi-Synchronous: Writes are considered successful only after updates are propagated to at least one other Replica via Synchronous Replication - this offers a good balance between Speed and Consistency.

							Chain-Replication

						Replication Log:
							Implementation:
								SQL Statements (for Relational Databases): Not ideal as SQL Statements can be non-deterministic (e.g. reference Current Time, which can be different per Replica).
								Write-Ahead Log Shipping: Describes which Bytes were changed. Not ideal as different Instances might have the affected Data in different Locations, and Rolling Upgrades (running different Database Engine versions amongst Replicas in the same Cluster) might not be possible.
								Logical Log: Describes which Data was modified and how. Better future-proofing if underlying Database Engine changes.
								Trigger-based Application Handling: Handle the Replciation at the Application Layer and not via the Database Engine. This can be done via Triggers, which execute supplied Application Code whenever a Write Operation is executed by the Database Engine. However, this typically incurs large overheads.

						Strategies:
							Single-Leader: All Writes to one Leader. Reads can come from any Replica (Follower / Leader) in the Cluster.
								Characteristics:
									Leader sends list of updates to other Followers via a Replication Log.

									Adding a Follower: Initialize the Follower with a consistent snapshot of the Data in the Leader (associated with some position in the Replication Log), and then update to the latest State via the Replication Log.

									Pros:
										Single Source of Truth => no Conflicts to resolve.
									
									Cons:
										All Writes going to a single Instance, so Write Throughput is lower.
											Can consider Sharding, and having different Leaders for each subset of the Data.

								Scenarios:
									Follower Failure: On reboot, fetch all changes from the Leader, and implementing the changes starting from its last position in the Replication Log.

									Leader Failure / Failover: Determine a new Leader based on some Consensus / most up-to-date Follower, and configure downstream Dependencies to send Writes to this new Leader + configure all other Follower to get the changes from the new Leader.
										Problems:
											Unreplicated / Partially Replicated Writes of old Leader to Followers.
											Accidental Failovers from temporary Network Congestion.
											Split Brain: Old Leader associates itself to still be a Leader upon recovery and accepts Writes, leading to an inconsistent Data State across the Cluster.

							Multi-Leader: Writes to one of many possible Leaders (possibly across Data Centers) - Leaders send each other Writes through some existing topology. Reads can come from any Replica (Follower / Leader) in the Cluster.
								Characteristics:
									Leaders can send each other Writes via several Topologies:
										Circular: A ring consisting of Leaders - each Leader passes Writes down to the next Leader after it in the ring. Not very Fault Tolerant.
										Star: Akin to a starfish - every Leader is connected to some central Node. Single Point of Failure.
										All-to-All (Fully Connected) - Each Leader is connected to every other Leader.

										Chain Replication?

									Pros:
										A Leader in each Data Center makes it easier to manage globally available Services. This model can also be used for Applications that require offline Edit access, or Applications that feature Real-Time Collaborative Editing, as the scenarios have similar characteristics.
										Write Throughput not limited by a single Replica.

									Cons:
										Write Conflicts possible between multiple Leaders.

								Scenarios:
									Detecting Concurrent Writes: Writes are considered Concurrent if each Source of the Write did not know of the Write from the others (compare with Causal Writes), NOT about time at which the Writes took place (i.e. it's all about keeping track of what the Source sees before making the Write).
										Version Vectors: Keep track of the number of Writes that each participant (Client / Replica) *thinks* all Replicas have made for every Object receiving Writes. 
											Each Replica persists their view of the Version Vector, and passes it to the Client, which it then passes back to the Replica during a Write for tracking, and update the Version Vector before passing it back to the Client again.

											Between two Version Vectors:
												If all Elements of one Version Vector is equal or greater than all corresponding Elements of another Version Vector, then it means that the Write for the first Version Vector happened after the other (Writes have the Happen-Before Relationship).
												If one Version Vector has Elements that are either smaller or greater than the corresponding Elements in the other Version Vector, then it means Concurrent Writes have occurred.

											Version Vectors are merged when the Consistency Model is triggered by taking the maximum of each Element between each Version Vector tracked per Replica.

									Conflict Resolution Strategies:
										Conflict Avoidance: Have all Writes for an Object go to one Leader. Not good for flexibility in Configuration (e.g. during Leader failure, or Horizontal Scaling).

										Last Write Wins: Each Write is given a Clock Timestamp, with the latest one being kept. Easy to implement, but Writes will be lost (important for use-cases requiring Version Histories) + possibility of Clock Skew between different Servers.

										On Read: Store all conflicting Writes for an Object. During a Read for the Object, resolve them and store the resolved value of the Object.

										On Write: Database relies on Application-defined Commutative (ordering doesn't matter) and Associative (which binary combinations don't matter after all are processed) Merging Logic to resolve Write Conflicts (Conflict-Free Replicated Data Types e.g. Distributed Counters, Decrementors, Distributed Sets, Ordered Lists).

							Leaderless: Writes to / Reads from all Replicas in parallel, and waits for a certain number of Replicas to return successfully (i.e. Quorum) before deeming operation a success.
								Characteristics:
									Quorum:
										Let W be the number of Replicas that need to succeed for a Client to consider their Write to be successful.
										Let R be the number of Replicas that need to succeed for a Client to consider their Read to be successful.
										Let N be the total number of Replicas in the Cluster. Typically N should be an odd number.

										Pseudo Strong Consistency Quorum: If W + R > N where N is the total number of Replicas, then at least one Replica will have the most updated Object.
											Typically W = R = (N+1)/2, then each Read / Write can tolerate up to (N-1)/2 Replicas failing per Operation.

											Imperfect:
												Allows for inconsistent Data States (i.e. Client considers Write failed as w < W, but w Replicas now have a potentially unwanted Write).
												Replica restoration after failure may take Data from another Replica that's more stale.
												During Sloppy Quorums, the Pseudo Strong Consistency Property no longer holds.

									Bookeeping Operations necessary in order to maintain Consistency between Replicas:
										Read Repair: On a Read, update those Replicas that has outdated Data with the Object with the highest Version Number (Eventual Consistency).

										Anti-Entropy: Background Process that occasionally looks at all Replicas to check for inconsistent versions for each Object's Version Number (Eventual Consistency / Bounded Staleness).
									
									Pros:
										Works well with Cross Data Center configurations by having W be small enough such that all Writes can go to one Data Center.
										Replication Strategy with the best flexibility for adjusting Consistency and Availabilty.

									Cons:
										Inconsistent Data still possible, even with a proper Quorum.
										Write Conflicts possible between multiple Replicas.
										Reads are slower + possibility of needing to do Read Repair.

								Scenarios:
									Conflict Resolution: See Multi-Leader Replication Section.

									Sloppy Quorums: Occurs when Replicas are temporarily added after the initial set of Replicas have all failed to handle further Writes - need to bring Written Data over to the initial set of Replicas once they have recovered (Hinted Handover).

				Optimization Techniques:
					Indexes:
						Background:
							The benefit of the Index is that it is able to reduce the number of Disk Block accesses maximally required to find the desired Record, especially with larger Collections.
								Index Entries are stored in some sorted order; more efficient Searches can be used that only work on sorted Data.

						Approach:
							A Primary Index is created for each Collection. For each Record, a Primary Key is used / generated via a UUID, and is added to the Primary Index such that the Primary Key is linked to the complete Record.
							Any other Indexes created will link the Fields used in the Index to the Primary Key(s), which is then used alongside the Primary Index to find the complete Record(s).

						Operations: For a given Key k (i.e. Fields) to return Value(s) v (i.e. Full Record / Location on Disk):
							insert(k, v)
							delete(k)
							[v1, v2, ... ] = query(k)
							[v1, v2, ... ] = range(k1, k2)

						Types:
							Hashmaps:
								Pros:
									Allows access to Records in Constant Time.

								Cons: 
									Entire Hashmap has to be In-Memory for it to be effective - useful for only small Datasets.
									Not good for Range Queries, as Data within the Index is unsorted.

							B Trees / B+ Trees: An extension of a Binary Search Tree - m-way Search Tree, but with constraints on how new Elements are inserted to prevent degenerate Trees.
								Approach:
									Each Node in the Tree is stored in one Disk Block.

									In the Non-Leaf Nodes, Keys of the Records used in the Index are stored alongside the pointers to next Node / Disk Block.
										For very large Collections, a Multi-Level Index may be used to avoid sequential Disk Block lookups in the original Index.
											At Higher Levels, the Keys stored in a Node / Disk Block becomes more sparse:
												Level 1: Keys 1 2 3, 4 5 6, 7 8 9, 10 11 12, 13 14 15, ...
												Level 2: Keys 1 4 7, 10 13 16, ...
												Level 3: Keys 1 10 19, ...

									Complete Records are only stored at the Leaf Nodes of the Primary Index; the Leaf Nodes of other Indexes map the Key Fields to the Primary Key(s), which can be used with the Primary Index to get the complete Record(s).

								Behaviour:
									Time Complexities:
										- Search / Update: O(logn)
										- Insertion: O(logn)
										- Deletion: O(logn)
									
									B-Tree Properties: 
										- Each Node can now store up to m-1 Elements, and m references to Children Nodes (m Degree / Order).
											- References to Child Nodes can be visualized as being stored 'between' the Elements.
											- Element Order in each Node is important.
										
										- Self-Balancing Properties:
											- All Non-Root Nodes with Children should have at least ceil(m/2) Child Nodes.
												- A Child Node can only be created if this property is fulfilled for the given Non-Leaf Node.

											- All Leaf Nodes should have at least ceil(m/2) - 1 Elements.

											- All Leaf Nodes (Nodes with no Children) should be at the same Level.
												- (Proactive) Insertion: Advantageous of not visiting a Node twice.
													- Bottom-Up: Tree grows upwards ; actual Element Insertion should only occur at a Leaf Node.
													- Traverse down from the Root Node to a Leaf Node.
														- If a given Node during Traversal is full, split the Node into 2 Child Nodes, and take the median Element from the original Node and set it to a Parent Node.
															- Left Bias: Resultant Left Child will have more Elements.
															- Right Bias: Resultant Right Child will have more Elements.

														- Continue Traversal from one of the Child Nodes.
												
												- (Proactive) Deletion: 
													- Traverse down from the Root Node.
														- If Element is in Non-Leaf Node:
															- If the Child Node that precedes / succeeds the Element has at least ceil(m/2)-1 Elements:
																- Obtain the Predecessor / Successor of the Element, depending on which Child Node has enough Elements.
																- Replace Element with Predecessor / Successor Element.
																- Delete Predecessor / Successor - guaranteed to be in Leaf Node.
																
															- Else: Both Child Nodes has less than ceil(m/2)-1 Elements:
																- Merge Both Child Nodes together, set Element to be Median Key of Child Node.
																- Traverse to new Child Node, and recursively delete Key from Child Node.

																- Note: Parent may have less than ceil(m/2)-1 Elements as a result of this operation, but this will be resolved in subsequent Deletes.

														- Else: Element is not in Current Non-Leaf Node.
															- Determine next Child Node to traverse to.

															- If Child Node has less than the minimum ceil(m/2)-1 Elements:
																- If an immediate Sibling Node has at least ceil(m/2) Elements, execute a left or right Rotation on one of the neighbouring Parent Elements.
																	- Send the neighbouring Parent Element down to the Child Node.
																	- Set the Predecessor or Successor Element of the Sibling Node in the Parent Element's position.
																- If both immediate Sibling Nodes have less than ceil(m/2) Elements, merge one of the Siblings together with the Child Node, with one the Parent Element becoming the Median Element of the Child Node.

															- Continue Traversal.
															- If Traversal has reached a Leaf Node, delete Element from Leaf Node if it exists.

										- A subset of the Tree can be stored in Cache, allowing for faster Query access.

									B+ Trees Properties:
										- Does not need to traverse the whole Tree for Sequential Access:
											- All Non-Leaf Nodes have their Elements in the Leaf Nodes as well.
											- All Leaf Nodes will be connected together like a Linked List from left to right.

								Pros:
									Higher Read Throughput - Sub-Linear Time Tree Traversal.
									Good for Range Queries (that only use the Keys in the Index), as the Data in the Index is internally Sorted.

								Cons:
									Relatively Slow Writes, as multiple Disk Blocks are accessed after each Write to update the Index.
	
							B-Epsilon / Fractal Tree Index: An extension of B Trees.
								Approach:
									- Each Non-Leaf Node now has a large buffer that accumulate Writes.
										- The buffer is part of the Disk Block that the Node resides in.
											- Each Write is linked to a Timestamp as well to maintain correct Operation Order.

										- The Root Node receives the Writes first.
											- Deletions are "negative" Writes implemented with Tombstone Messages.

									- When a Node's buffer is full, it will flush some of the accumulated Writes down to its Child Nodes.
										- Transfer of accumulated Writes are batched, which saves on repeated Disk Block accesses for each Write (from Root Node to Parent Node).
											- Compared to regular B-Trees where Writes would each individually require 2 Disk Block accesses vs transferring all Writes at one go (using Memory as an intermediary).

											- Writes are only flushed if the Child has enough pending Writes to offset the cost of re-writing the Parent and Child Nodes.

										- If all Writes are to go to the same Leaf Node, optimizations should be made to immediately flush the Writes (and any other in the Path) directly to the Leaf Node, instead of adding and removing the Writes from each intermediate Node from the Root to the Leaf.

									- A Write is fully processed when it reaches a Leaf Node.

									- Fractal Tree differences:
										- Node Buffers are also Indexed themselves.
										- Compatibility with ACID Transactions.
										- Cleaner Threads that occasionally flush buffers when Database is idle.

								Pros:
									Higher Write Throughput compared to B-Trees - batching Writes allow for Larger Nodes (multiples of Disk Block size) vs. regular B Tree Nodes, in terms of Write Cost.
										Larger Nodes allows for more effective Compression vs. Smaller Nodes in regular B Trees.

									Better Performance on Upsert Operations than B Trees.

								Cons:
									Asymptotically same Read Performance, but is actually up to 2x more Disk Block Accesses.
									Equivalent performance to B-Trees when checking for unique Index Keys.

							Geospatial:
								Approach:
									Point-Region Quad Tree; the Key of each Point is of variable length, depending on how "deep" it is within the Tree.
										Keys are Binary, with 2 Bits appended per "level".
											00 - Top Left
											01 - Top Right
											10 - Bottom Left
											11 - Bottom Right

										When a Node is full, the contents of the Node should be redistributed to 4 Child Nodes.
											When a Node is full is determined by the Application.
											Keys of Child Nodes should have the Keys of their Parent Nodes prefixed to theirs.
												Example: 010010
													Point is located in the Bottom Left of the Top Left of the Top Right Region.

					Partitioning: Splitting up a large Collection into smaller non-overlapping Partitions such that each Partition can be stored and maintained in separate Nodes. Each Node can contain more than one Partition.
						Partitioning is usually only considered when a Collection becomes too big to be stored in one Node, as it adds a lot of underlying complexity to the System.
							Gossip Protocol (Database Nodes communicate between themselves)
							Layer to track which parts of the Dataset is stored in which Partition / Node.

						Often used alongside Replication (i.e. Nodes have separate Replica Sets).

						Objectives:
							Similarly sized Partitions (not necessarily equal in size as there might be certain Partitions that get more attention normally than others).
							Overall Traffic distributed equally per Node.

							Nodes become "Hotspots" if these are not met for any Node in the Partition Set.

						Configurations:
							Fixed Number of Partitions:
								Too Low:
									Each Partition will get too big, and Application can't scale.
									Rebalancing will take a longer time.

								Too High:
									Larger Overhead on Disk for each Node w.r.t keeping track of all Ranges and Disk Locations for each Partition.

							Fixed Number of Partitions per Node:
								Each Node has a certain number of Partitions that grow proportionally to the Dataset.
									Upon adding a new Node it will split existing Partitions from each Node, and take a piece of each split for itself.

							Dynamic Partitioning: Some Databases are able to adjust Partitions automatically to reduce Hotspots as Data Access Patterns change over time.
								Splitting and Merging of Partitions.

						Methods:
							Ranges: Each Partition involves a Range which is a Contiguous Ordered Sequence of Values. Ranges can be of different sizes to avoid Hotspots.
								Of Keys: Use Record Keys as they are to determine which Partition / Range they belong to (e.g. Alphabetical Ranges).
									Pros:
										Simple to understand.
										Compatible with Range Queries.

									Cons:
										Need to actually ensure Ranges meet Objectives (e.g. Timestamp Ranges can lead to Hotspots).

								Of Hashes of Keys: Hash Record Keys before determining which Partition / Range they belong to.
									Pros:
										Assuming a good Hash Function, Keys are evenly distributed.

									Cons:
										Not ideal for Range Queries.
										Hotspots can still occur if a few Record Keys are always requested for.

						Rebalancing: Should attempt to keep a majority of the Records in the same Node to avoid high Bandwidth costs.
							Use Consistent or Rendezvous Hashing to ensure minimal Record disruption whenever a Partition is added or removed.

							During the process the old Node will take on the Reads and Writes for the Data involved in the migration to the new Node.

						Compatability with other Database Mechanisms:
							Secondary Indexes: In both cases, the Secondary Index is to be split across Partitions, in the same way that the Collection is to be split across Partitions. The distinction is regarding what the Partition Key for the Secondary Index should be (i.e. the Partition Key for the Index can be different than the Index Key, e.g. an Index created on the 'Country' Field in a Table to speed up queries relating to 'Country', but the Table's chosen Partition Key is 'UserID'), and whether or not the Secondary Index is housed in the same Partitions as the Collection or is housed in a set of Partitions separate from the Collection's Partitions.
								Local Indexes: Set the Secondary Index to be split between Partitions according to the same Partition Key as the Collection it is tracking. This means that the Secondary Index within each Partition will only track Record Keys within the Partition.
									Pros:
										Fast for Writes since all Writes are within the Partition.

									Cons:
										Slower on Reads involving the Index Key, as all Local Indexes in all Partitions need to be aggregated / processed to fully determine which Partition(s) has the Record.

										Must be created when the original Table is created (only applicable for DynamoDB where the Partition Size is fixed; adding a Secondary Index later will necessitate rebalancing which DynamoDB avoids).

								Global Indexes: Set the Secondary Index to be split according to a different Partition Key (i.e. the Index Key that it was created on) from the Collection it is tracking.
									Pros:
										Faster for Reads involving the Index Key since all related Data for the Index Entry is stored in one Partition.

									Cons:
										Separate Partitions from the Collection's original Partitions required.
										Slow on Write, as Writes now potentially need to go to multiple Partitions (one for the Record, and one / two to update the Index Entry).
										Consensus or Transaction Mechanism required in case Write to Record succeeds and Update to Index Entry (on a different Partition) fails.

				Consensus:
					Two-Phase Commit: Consensus Protocol that allows for Atomic Commits (i.e. Operations within a Transaction either all succeed or fail on all involved Nodes).
						Setup:
							A Coordinator Process / Node is required.
							Every Transaction should have a monotonically increasing Transaction ID.

						Phases:
							Request Phase:
								The Coordinator sends a Query to Commit Message once all Nodes have received their set of Operations.
								The Nodes executes the Operations up to the Commit Point.
									They additionally update their Undo and Redo Logs, in case one of their siblings cannot continue.

								The Nodes replies to the Coordinator an "Ack" or "Abort" Message, depending on successful or failed execution of the Operations respectively.

							Commit Phase:
								Success: 
									Once the Coordinator receives "Ack" from all Nodes involved in the Transaction, it will send a "Commit" Message to the Nodes.
									The Nodes will then Commit the changes from the executed Operation, and release all Locks and Resources held during the Transaction.
									The Nodes will then send an "Ack" back to the Coordinator, which will then mark the Transaction as completed once it has received the response from all Nodes.

								Failure:
									Once the Coordinator receives an "Abort" from any of the Nodes involved in the Transaction, it will send an "Abort" Message to the Nodes.
									The Nodes will then undo the Transaction using the Undo Log, and release all Locks and Resources held during the Transaction.
									The Nodes will then send an "Ack" back to the Coordinator, which will then mark the Transaction as aborted once it has received the response from all Nodes.

						Pros:
							Allows for Atomic Commits.
							Simple Consensus Mechanism.

						Cons:
							Not Fault Tolerant as the Coordinator is a Single Point of Failure.
								Alleviated by allowing for Relicated Coordinators, though depending on configuration this may slow down performance.

							If both the Coordinator and a Node fails, it will not be possible to determine if the Commit Phase was initiated with what state, as the Node that failed could have received the Commit Phase Message.
								Alleviated by using Three-Phase Commit, which introduces an additional Prepare-to-Commit Phase.
							
							Blocking: Further Operations cannot proceed until the Coordinator / all Nodes have succeeded in executing their Operations.

					Raft: Consensus Protocol used in Single Leader Replication to automatically elect a new Leader during Failover and build Replication Logs (achieving Total Order Broadcast).
						Setup:
							Each Replica maintains a Term (Fencing Token).
							Each Replica maintains its own Log of Database Operations performed (a.k.a Replication Log), and each Operation is associated with a Term.
						
						Normal Behaviour:
							Each Replica is either in a Follower, Candidate or Leader State:
								Follower:
									A Replica that awaits to receive compatible updates in the form of Heartbeat Messages from a Leader.

									All Replicas are Followers upon startup.

									The moment any Replica receives a Message from another Replica that has a higher Term, the former must become a Follower and update its own Term to match.

								Candidate:
									A Follower upgrades itself to a Candidate State when it has not received a Heartbeat Message with the same or greater Term Number / a Vote Request with a greater Term Number after a certain amount of time (randomized for each Follower to avoid scenarios where it becomes impossible to form a Quorum).
										It will increase its Term by 1, and send Election Requests to the rest of the Followers whilst voting for itself.
											The content of an Election Request contain the index and Term of the last Operation that the Follower has commited thus far.

										Scenarios:
											If a Replica has already cast a vote for the new Candidate this Term (either for itself as a Candidate, or another Candidate), it will not vote for other Candidates.

											If a Replica receives an Election Request from a Candidate with a higher Term, in addition to becoming a Follower and updating its own Term to match:
												If the Election Request comes from a Candidate with a more updated Log, the Follower will cast the vote for the Candidate.

											Else it will not vote for the Candidate that sent the Election Request.

										Outcomes:
											If a Quorum is reached in its Favour, it will immediately send the next Heartbeat Message to all other Followers, self-declaring that it is the new Leader.

											If a Quorum is not reached in its Favour, it will remain as a Candidate till it either receives a Heartbeat Message from a new Leader or its Election Timeout expires, afterwhich it will re-elect itself as a Candidate, repeating the Election Process.

								Leader:
									A Candidate upgrades itself to a Leader once it has received votes in its favour from a Quorum of the other Replicas.

									The Leader must attempt to track the Prefix and Suffix for each Follower:
										Prefix: An estimate on the Operations that the Follower has successfully received.
										Suffix: An estimate on the Operations that the Leader has sent to the Follower, but has not been successfully received.

									A Leader would propagate updates to its Followers one at a time, even if there are multiple in its queue:
										The Heartbeat Message from the Leader containing the update has the following information:
											The current Term of the Leader.

											The index and Term of the last Operation in the Prefix of the Follower.
										
											The first Operation in the Suffix of the Follower. 

											The estimated number of Commits that the Follower has made.

										The Leader executes the Operation but does not commit it, instead propagating the Operation to its Followers first.
											Outcomes:
												If the Follower's Term of the last Operation in the Prefix matches the one in the Heartbeat Message, the update will be successful.
													Any Operation that exists in a majority of the Replicas will take precedence, and eventually be committed, otherwise no guarantee.
												
												Else, the Leader will have to keep reducing the Prefix and increasing the Suffix until the update succeeds, or become a Follower itself if the Follower has a higher Term.

										The Followers execute the Operation but do not commit it, responding back to the Leader upon success.

										The Leader waits to receive successful responses from a Quorum, and proceeds to commit the Operation.
										The Leader then sends the commit request to its Followers, making them commit the Operation.

										The entire process repeats for each subsequent update.
							
						Special Scenarios:
							There can be more than one Candidate during an Election.
							A Leader will become a Follower if another Follower that did not manage to receive Heartbeat Messages becomes a Candidate and increments its Term.

							If the Replica Set suffered a Network Partition such that the Replica Set was split into two groups and the existing Leader is in a group that is smaller than the other, then there will eventually be two Leaders. However, the old Leader won't be able to commit Operations because it won't have access to a Quorum.

						Pros:
							Fault Tolerant.

						Cons:
							Not usable for achieving Consensus in Multi-Leader / Partitioned Setups; strictly for Single Leader Replication.

							Blocking: Further Operations cannot be committed until the oldest uncommitted Operation succeeds in being committed.

		Data Models:
			Relational: Models Data in terms of Tables containing Rows of Structured Data, with Rows in one Table having a Relation to Rows in another if both share common Key(s).
				Characteristics:
					Reduction of Data Duplication via Normalization and JOINing.

					Each Record stores its Values contiguously on Disk.
					Each Table has one Schema, which eases Data Encoding.

				Pros:
					Row-Based Storage Strategy optimal for querying many Columns at once per Record (unless Vertically Partitioned).
					Relational Structure allows minimization of Data Duplication across Tables (Many-to-One Relationships).

				Cons:
					Row-Based Storage Strategy not optimal for Data Compression.
					Row-Based Storage Strategy not ideal for Aggregate Queries (alleviated if Aggregate Query performed on an Indexed Attribute).

					Scalability:
						Challenging to Partition since (Distributed) Transactions have to ensure that they succeed or fail as a whole, which can involve multiple Nodes and even more Network Calls.
							JOINs / Row Splitting would require aggregation across Nodes.
							Distributed Transactions required for Writes.

					Strict Schema does not allow Data to flexibly adapt to changing Application needs quickly.

				Related Content:
					SQL
					NewSQL:		Seeks to combine the strengths of Relational Databases with the strengths of Document Databases.

					Data Warehouse: A Secondary Database that stores computed Data from a Production Database in a way that is more efficient for long running Analytical Queries.

				Examples:
					MySQL
					PostgresSQL

					NewSQL:
						VoltDB
						Google Spanner
							All Servers have an attached GPS Clock, which provides correct measurement of Event order.

					PrestoDB:	Facebook's Distributed SQL Query Engine.

			Non-Relational: Anything that allows storage and retrieval of Data via other means besides the Strict Tabular Relations used in SQL Databases.
				Types:
					Document: Models Data as Trees of Key-Value Pair Objects.
						Characteristics:
							Documents are / supposed to be self-containing; they group relevant Data together (One-to-Many Relationship).
								Storage Locality: Documents are stored as a single continuous string with a particular encoding (i.e. JSON, XML).
								Operations are guaranteed to be Atomic on single Documents.

						Pros:
							Schema not required; Data can evolve more flexibly.
							Scalability: Easier to Horizontally Partition due to self-containing nature of Documents.

						Cons:
							Design can lead to Data Duplication (Many-to-One Relationship).

							Not suited for scenarios where there are many Relations between Data, since Documents are supposed to be self-containing.

							Storage Locality is only useful in cases where large parts of the Document are loaded / modified at a time - usually the entire Document needs to be loaded / recreated per Edit.

						Examples:
							MongoDB
							CouchDB: Capable of Multi-Leader Replication vs. MongoDB.
							Rethink DB: Supports Joins in its Query Language.
							DynamoDB:	Amazon's NoSQL Solution.
							Firestorm

					Graph: Models Data in a Graph (i.e. Vertices and Edges, or Triples (e.g. Subject, Predicate, Value / Subject)).
						Characteristics:
							Vertices need not conform to any one Schema, allowing for Relationships between different Entities.
							Does well for Data with Many-to-Many Relationships.

						Pros:


						Cons:


						Examples:


					Key-Value Database: Uses a Map-capable / Associative Array Data Structure to map an Item's Key to the Item itself. Suited for storing / caching simple Items.
						Characteristics:
							The Item's Key may be automatically generated whenever an Item is saved to the Database.

						Pros:
							As every Item has no strict Schema to adhere to, memory can be saved by not having to track "placeholders" for Fields which would not be in use for some Items, vs. Relational Databases.

							Databsase can be Partitioned with Consistent Hashing.

						Cons:
							

						Examples:
							Redis: In-Memory based Database with Disk Persistence that is often used as a Caching Layer on top of other RDBMSes.
							Project Voldemort
							Riak
					
					Wide-Column Database: Stores Data arranged into Columns rather than Rows.
						Characteristics:
							Each Disk Block stores the Values for one Attribute within multiple Records.

						Pros:
							Column-Based Storage Strategy optimal for performing Aggregate Queries.
						
						Cons:
							Column-Based Storage Strategy incurs Heavy Performance Cost when accessing multiple Attributes for a given Record.

							Limited Indexing.

						Examples:
							Apache Cassandra: Implementation with emphasis on Scalability and Fault Tolerance.
								Uses Timestamps to measure ordering of Events; recommends keeping Servers updated with NTP as much as possible.

							ScyllaDB: Faster, Sharded Alternative to Apache Cassandra.

					Time-Series Database: Specialized for storing, querying and indexing Data generated at fixed intervals, or at discrete points in time.
						Characteristics:
							Prioritises Write (Create) Performance - LSM Trees for Primary Index.

							Schemaless - New Events can change formats seamlessly without having to update Older Events.

							Stores Data in such a way that Data of the same Type is stored together, allowing for optimal Data Compression.

						Examples:
							InfluxDB: Typically paired with Grafana for System Monitoring.

					Streaming Database: Specialized for processing continuous streams of Data in real-time.
						Characteristics:
							Prioritises real-time processing of incoming Data (e.g. windowing, filtering, joining).

						Examples:
							Apache Kafka: Message-Queue.
								Confluent Platform: Built on top of Apache Kafka to provide additional features.

							Apache Flink

							Redis Streams: Data Structure in Redis that allows for the storage and processing of real-time Data Streams.

							Amazon Kinesis
							Google Cloud Pub/Sub
							Azure Stream Analytics

		Other Characteristics:
			In Memory Database: Databases that keep their Data completely in RAM.
				These may still write Data to Disk Asynchronously for Weak-Durability Concerns (e.g. Append-Only Log), they are still considered In-Memory because the Data that they are meant to serve is still primarily from RAM.
				The performance advantage of In Memory Databases over traditional Disk Databases comes from avoiding the need to serialize the Data to a format compatible for a Disk Write, not from being able to avoid Disk Reads.

	Commands:
		
	
