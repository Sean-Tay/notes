Database
	Abbreviations: 
		DBMS:					DataBase Management System.

		CMDB: 					Configuration Management Database.

	Description: 
		A Database is an Organized, Long-Term Collection of Data, generally Stored, Managed and Accessed via some System.
	
	Resources:
	
	Concepts:
		Data Types:
			Structured Data:	Data whose Elements are Directly Addressible (i.e. Tabular or Array-like Data).
			Semi-Structured Data: Data whose Elements may not be Directly Addressible, but still contain other forms of Organizational Properties (i.e. Tagged Data: XML).
			Unstructured Data:	Data which is currently observed to have no immediately helpful Pre-Defined Format (i.e. Binary Data: PDFs).

		Attribute / Key / Field: Refers to a characteristic of some type of Entity.
		Record:					A collection of Attributes pertaining to some instance of an Entity.
		Collection:				A group of Records.

		Forward Index:			A Data Structure that maps the Record to the Record's Content.
		Inverted Index: 		An additional Data Structure (separate from the Collection) that allows for quick access of a Full Record on Disk via a defined subset of the Record's Data.
			Sparse Index:		Contains a mapping only for every n Records in the Collection to the Disk Location. Typically used in Multi-Level Indexing (i.e further Indexing of a Dense or overly Large Index).
			Dense Index:		Contains a mapping for every Record in the Collection to the Disk Location.

		Database Partitioning:
			Horizontal Partitioning: Different Records into Separate Collections (e.g. Sharding).
			Vertical Partitioning: Different Record Attributes into Separate Collections (e.g. Normalization or simple Record Splitting).

		Replication:			Having Redundant Copies of the Data in a Database stored in multiple Machines so that if one fails, dependent Applications can still use the other Machines for Data Retrieval, achieving Reliability for the Distributed System. Also helps to reduce operational load per Instance.
		Replica:				A Machine within a Replicated Database.

		CAP Theorem: 			Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3). 
			Consistency:		Degree at which Data is the same throughout the Database.
				Consistency Model: Defines a contract in implementation to guarantee certain properties of Reads / Writes to Data in a Database. One or more can be implemented to achieve greater Consistency.

			Availability:		Whether or not every Request for Reading / Writing Data is successful.
			Partitioning Tolerance: Enforces that the Database can still operate even if parts of it are unreachable in the Network.

	Details:
		Database Objectives:
			Persistent Data Storage: Data should not be lost when the Database shuts down.

			Available Writes
			Available Reads
				Data should be stored contiguously on Disk to minimize the amount of Disk Lookups required (known to be expensive as it involves mechanical movement of the Storage Disks at the Hardware level).

				Optimization Techniques:
					Append-Only: New Records / Updated-versions of Past Records are simply appended to the end of the Collection without concern for Duplication. Lookups can be optimized by traversing from the end of the Collection.

					Indexes: 	Improves speed of Lookups, but imposes additional operations during Insertions / Deletions, and adds to the Disk Space required by the Collection.
						Background:
							The benefit of the Index is that it is able to reduce the number of Disk Block accesses maximally required to find the actual Disk Block that stores the complete Record, because only the necessary subset of the Record's Data is stored in each Disk Block used by the Index (thus each Disk Block can store more Block Addresses).
								The optimization is more effective when Records are large => Less Records per Disk Block => More Disk Block Lookups without an Index.

							The Data within the Index is usually stored in sorted order, but the actual Records in the Collection on Disk may not be stored in sorted order.

						Types:
							Hashmaps: Index Keys map to Memory Addresses (Byte Offsets) in Disk.
								Pros:
									Compatable with the Append-Only method of adding new Records to the Collection.
									Allows access to Records in Constant Time.

								Cons: 
									Entire Hashmap has to be In-Memory for it to be effective - useful for only small Datasets.
									Not good for Range Queries, as Data within the Index is unsorted.

							LSM Trees + Immutable SS Tables: 
								Approach:
									Full Records realized from Database Writes are first placed in an In-Memory Balanced Binary Search Tree, with the Key used in the BST being the Index Key(s).
										To increase persistence, keep a Log on Database Writes that can be used to restore the In-Memory BBST after a Database Outage.

									When Tree gets too large, a Sorted SS Table is generated via BST In-Order Traversal and is transferred to Disk.
										The Tree gets emptied each time a SS Table is generated.

									There can be duplicate Keys between multiple Tables (and the Tree itself) - simply prioritise looking for the Key in the Tree, followed by the youngest SS Table to the oldest.
										Sparse Hashmaps with Memory Addresses + Binary Search on Table Keys can be additionally used to optimize lookups within an SS Table.
										Bloom Filters can be additionally used to optimize lookups between SS Tables.

										Tables can be combined in the same approach that Merge Sort uses to combine 2 Sorted Lists - Linear Time Background Operation.
											Prioritise the Key-Value Pair that is in the younger SS Table over the older SS Table.

								Pros:
									Higher Write Throughput due to Records being placed In-Memory at first.
									Uses the Append-Only method of adding new Records to the Collection.
									Good for Range Queries (that only use the Keys in the Index), as the Data in the Index is internally Sorted.

								Cons:
									Relatively Slow Reads for Keys that do not exist in the Collection.
									Merging Process for SS Tables can take up Background Resources.

							B Trees / B+ Trees: An extension of a Binary Search Tree - m-way Search Tree, but with constraints on how new Elements are inserted to prevent degenerate Trees.
								Approach:
									Each Node in the Tree is stored in one Disk Block.

									In the Non-Leaf Nodes, Keys of the Records used in the Index are stored alongside the pointers to next Node / Disk Block.
										For very large Collections, a Multi-Level Index may be used to avoid sequential Disk Block lookups in the original Index.
											At Higher Levels, the Keys stored in a Node / Disk Block becomes more sparse:
												Level 1: Keys 1 2 3, 4 5 6, 7 8 9, 10 11 12, 13 14 15, ...
												Level 2: Keys 1 4 7, 10 13 16, ...
												Level 3: Keys 1 10 19, ...

									Complete Records are only stored at the Leaf Nodes.

								Behaviour:
									Time Complexities:
										- Search / Update: O(logn)
										- Insertion: O(logn)
										- Deletion: O(logn)
									
									B-Tree Properties: 
										- Each Node can now store up to m-1 Elements, and m references to Children Nodes (m Degree / Order).
											- References to Child Nodes can be visualized as being stored 'between' the Elements.
											- Element Order in each Node is important.
										
										- Self-Balancing Properties:
											- All Non-Root Nodes with Children should have at least ceil(m/2) Child Nodes.
												- A Child Node can only be created if this property is fulfilled for the given Non-Leaf Node.

											- All Leaf Nodes should have at least ceil(m/2) - 1 Elements.

											- All Leaf Nodes (Nodes with no Children) should be at the same Level.
												- (Proactive) Insertion: Advantageous of not visiting a Node twice.
													- Bottom-Up: Tree grows upwards ; actual Element Insertion should only occur at a Leaf Node.
													- Traverse down from the Root Node to a Leaf Node.
														- If a given Node during Traversal is full, split the Node into 2 Child Nodes, and take the median Element from the original Node and set it to a Parent Node.
															- Left Bias: Resultant Left Child will have more Elements.
															- Right Bias: Resultant Right Child will have more Elements.

														- Continue Traversal from one of the Child Nodes.
												
												- (Proactive) Deletion: 
													- Traverse down from the Root Node.
														- If Element is in Non-Leaf Node:
															- If the Child Node that precedes / succeeds the Element has at least ceil(m/2)-1 Elements:
																- Obtain the Predecessor / Successor of the Element, depending on which Child Node has enough Elements.
																- Replace Element with Predecessor / Successor Element.
																- Delete Predecessor / Successor - guaranteed to be in Leaf Node.
																
															- Else: Both Child Nodes has less than ceil(m/2)-1 Elements:
																- Merge Both Child Nodes together, set Element to be Median Key of Child Node.
																- Traverse to new Child Node, and recursively delete Key from Child Node.

																- Note: Parent may have less than ceil(m/2)-1 Elements as a result of this operation, but this will be resolved in subsequent Deletes.

														- Else: Element is not in Current Non-Leaf Node.
															- Determine next Child Node to traverse to.

															- If Child Node has less than the minimum ceil(m/2)-1 Elements:
																- If an immediate Sibling Node has at least ceil(m/2) Elements, execute a left or right Rotation on one of the neighbouring Parent Elements.
																	- Send the neighbouring Parent Element down to the Child Node.
																	- Set the Predecessor or Successor Element of the Sibling Node in the Parent Element's position.
																- If both immediate Sibling Nodes have less than ceil(m/2) Elements, merge one of the Siblings together with the Child Node, with one the Parent Element becoming the Median Element of the Child Node.

															- Continue Traversal.
															- If Traversal has reached a Leaf Node, delete Element from Leaf Node if it exists.

									B+ Trees Properties:
										- Does not need to traverse the whole Tree for Sequential Access:
											- All Non-Leaf Nodes have their Elements in the Leaf Nodes as well.
											- All Leaf Nodes will be connected together like a Linked List from left to right.

								Pros:
									Higher Read Throughput - Sub-Linear Time Tree Traversal.
									Good for Range Queries (that only use the Keys in the Index), as the Data in the Index is internally Sorted.

								Cons:
									Relatively Slow Writes, as multiple Disk Blocks are accessed after each Write to update the Index.
	
				CAP Theorem: Describes the tradeoff between Data Consistency, Data Availability and Partitioning Tolerance for a Database (choose 2 of 3).
					Configurations:
						For a Distributed Database, Partition Tolerance is needed as Networks are unreliable, so either Consistency or Availability is compromised.
							If Consistency is prioritised, that means the Database must for e.g. prevent Write Requests from succeeding until all Replicas can be reached again.
							If Availability is prioritised, that means that the Data between Replicas can go out of sync during certain scenarios.

						For a Centralized Database need not have Partitioning Tolerance, so both Consistency and Availability is achievable.

					Consistency Models: In ascending order of resultant Consistency / descending order of resultant Availability:
						Weak Consistency: Each Read on an Object is only guaranteed to return the result of applying a subset of Writes to that Object thus far (e.g. Writes 2 to 3, Writes 1 to 4, Writes 1 & 3 to 5). This means that each Read has a chance to return a version of the Object that has never actually existed.

						Eventual Consistency: Weak Consistency in the short term, but will reach Strong Consistency in the long term.

						Consistent Prefix: Each Read on an Object is guaranteed to return the result of applying an ordered sequence of Writes on an Object thus far, starting from the first Write (e.g. Writes 1 to 3, Writes 1 to 2, Writes 1 to 5). This means that each Read will return a version of the Object that has actually existed at some point in time.

						Session Consistency: Data Abnormalities should not occur for Reads and Writes during a limited period of time.
							Monotonic Reads: Each Read in a sequence of Reads on an Object is guaranteed to return the result of applying at least the same subset of Writes on the Object thus far as the previous Read (e.g. Writes 3 to 4, Writes 3 to 4, Writes 1 & 3 to 4, Writes 1 to 5).
							Read My Write: The result of all Writes to an Object done by a Client is guaranteed to be observable in the Client's subsequent Reads done on the same Object (i.e. Strong Consistency only guaranteed for the Client).

						Bounded Staleness: Each Read on an Object is guaranteed to give the result of applying at least x Writes to that Object thus far (e.g. at least Writes 1 to 3, if x = 3 => Writes 1 to 3 & 5).

						Strong Consistency: A Read is guaranteed to return the result of applying all Writes on an Object thus far. Performance-intensive; depending on use case, it may be possible to use the weaker Consistency Models for the same effect. Note that this is technically Bounded Staleness with x = n.

					Replication:
						Types:
							Synchronous: Writes not considered successful until all Instances in the Cluster completes the Write Transaction. Strong Consistency, but slower Speed.

							Asynchronous: Writes considered successful when the Instance that was assigned the Write Transaction completes it. Faster Speed, but Clients may get stale / inconsistent Reads due to Eventual Consistency.
								Server / Database should simply return the written Data back to the Client that triggered the Write upon Success, or Read only from Leaders for editable Data (Read My Write Consistency).
								Each Client should only refer to one Instance for updates throughout the operation (Consistent-Prefix + Monotonic Reads Consistency), as different Replicas synchronize with a particular Write at different Speeds (Back-in-Time syndrome).
								Writes with Casual Relationships should ideally happen on the same Disk Partition (Consistent Prefix Reads), otherwise they might not make sense when they are Read (different Read Speeds for different Disk Partitions).

						Strategies:
							Single-Leader: All Writes to one Leader. Reads can come from any Replica (Follower / Leader) in the Cluster.
								Characteristics:
									Leader sends list of updates to other Followers via a Replication Log.
										Implementation:
											SQL Statements (for Relational Databases): Not ideal as SQL Statements can be non-deterministic (e.g. reference Current Time, which can be different per Replica).
											Internal Append-only Log: Describes which Bytes were changed. Not ideal as different Instances might have the affected Data in different Locations.
											Logical Log: Describes which Data was modified and how. Better future-proofing if underlying Database Engine changes.

									Adding a Follower: Initialize the Follower with a consistent snapshot of the Data in the Leader (associated with some position in the Replication Log), and then update to the latest State via the Replication Log.

									Pros:
										Single Source of Truth => no Conflicts to resolve.
									
									Cons:
										All Writes going to a single Instance, so Write Throughput is lower.
											Can consider Sharding, and having different Leaders for each subset of the Data.

								Scenarios:
									Follower Failure: On reboot, fetch all changes from the Leader, and implementing the changes starting from its last position in the Replication Log.

									Leader Failure / Failover: Determine a new Leader based on some Consensus / most up-to-date Follower, and configure downstream Dependencies to send Writes to this new Leader + configure all other Follower to get the changes from the new Leader.
										Problems:
											Unreplicated / Partially Replicated Writes of old Leader to Followers.
											Accidental Failovers from temporary Network Congestion.
											Split Brain: Old Leader associates itself to still be a Leader upon recovery and accepts Writes, leading to an inconsistent Data State across the Cluster.

							Multi-Leader: Writes to several Leaders (possibly across Data Centers) - Leaders send each other Writes through some existing topology. Reads can come from any Replica (Follower / Leader) in the Cluster. 
								Characteristics:
									Leaders can send each other Writes via several Topologies:
										Circular: A ring consisting of Leaders - each Leader passes Writes down to the next Leader after it in the ring. Not very Fault Tolerant.
										Star: Akin to a starfish - every Leader is connected to some central Node. Single Point of Failure.
										All-to-All (Fully Connected) - Each Leader is connected to every other Leader.

										Chain Replication?

									Pros:
										A Leader in each Data Center makes it easier to manage globally available Services.
										Write Throughput not limited by a single Replica.

									Cons:
										Write Conflicts possible between multiple Leaders (e.g. Write 2 coming before Write 1).

								Scenarios:
									Detecting Concurrent Writes: Writes are considered Concurrent if each Source of the Write did not know of the Write from the others (compare with Causal Writes), NOT about time at which the Writes took place (i.e. it's all about keeping track of what the Source sees before making the Write).
										Version Vectors: Keep track of the number of Writes that each participant (Client / Replica) *thinks* all Replicas have made for every Object receiving Writes. 
											Each Replica persists their view of the Version Vector, and passes it to the Client, which it then passes back to the Replica during a Write for tracking, and update the Version Vector before passing it back to the Client again.

											Between two Version Vectors:
												If all Elements of one Version Vector is equal or greater than all corresponding Elements of another Version Vector, then it means that the Write for the first Version Vector happened after the other (Writes have the Happen-Before Relationship).
												If one Version Vector has Elements that are either smaller or greater than the corresponding Elements in the other Version Vector, then it means Concurrent Writes have occurred.

											Version Vectors are merged when the Consistency Model is triggered by taking the maximum of each Element between each Version Vector tracked per Replica.

									Conflict Resolution Strategies:
										Conflict Avoidance: Have all Writes for an Object go to one Leader. Not good for flexibility in Configuration (e.g. during Leader failure, or Horizontal Scaling).

										Last Write Wins: Each Write is given a Clock Timestamp, with the latest one being kept. Easy to implement, but Writes will be lost (important for use-cases requiring Version Histories) + possibility of Clock Skew between different Servers.

										On Read: Store all conflicting Writes for an Object. During a Read for the Object, resolve them and store the resolved value of the Object.

										On Write: Database relies on Application-defined Commutative (ordering doesn't matter) and Associative (which binary combinations don't matter after all are processed) Merging Logic to resolve Write Conflicts (Conflict-Free Replicated Data Types e.g. Distributed Counters, Decrementors, Distributed Sets, Ordered Lists).

							Leaderless: 
								Characteristics:
									

								Scenarios:
									



		Database Types:
			In-Memory Database: 	A Database which stores Data in Main Memory, which is much faster than Disk Storage as I/O is skipped. Suitable for Operations in which Response Time is critical.
				Pros:


				Cons:


				Examples:

			Relational Database: Consists of Tables storing Rows of Structured Data.
				Pros:
					ACID-strengthened Transactions:
						Atomicity:		Transactions containing multiple Statements will either succeed as a whole, or not be processed at all.
						Consistency:	Transactions can only bring the Database from one valid State to another, respecting Constraints, Cascades and Triggers within affected Data.
						Isolation: 		Transactions executed concurrently will behave the same if the Transactions are executed sequentially.
						Durability:		Transactions that have been commited will remain commited, even during System Failure (Power Outage / Crash).

						Benefits diminish in Distributed Settings.

					Relational Structure allows minimization of Data Duplication across Tables.

				Cons:
					Scalability:
						CAP Theorem suggests that Availability, Consistency and Partition Tolerance cannot be satisfied at the same time.
						Row-Based Data Structure not optimal for Data Compression.

						Challenging to Horizontally Partition due to JOINs.
						Data may not always be stored in one Partition / Machine, and may require several Network Calls to fulfill.

				Related Content:
					SQL
					NewSQL:		Seeks to combine the strengths of Relational Databases with the strengths of Document Databases.

				Examples:
					PrestoDB:	Facebook's Distributed SQL Query Engine.

			NoSQL: Anything that is not storing Data in a Structured Table Format:
				Document Database:
					Pros:
						Performance:
							Data Locality minimizes the distance of Disk Lookups, as the Document is usually stored in one place on Disk, and in one Machine.

						Scalability:
							Easier to Horizontally Partition without ACID Considerations.

					Cons:
						Larger Network Calls as a result of sending over whole Documents with unneeded Information.

					Related Content:

					Examples:
						MongoDB
						DynamoDB:	Amazon's NoSQL Solution.
						Firestorm

				Graph Database: Stores Data in a Graph (i.e. Vertices and Edges). Queries involve traversing the Graph.
					Pros:
						Good for storing Heterogenous and Homogenous Types of Data.

					Cons:

					Related Content:

					Examples:
						Cassandra

				Key-Value Database:
					Pros:

					Cons:

					Related Content:

					Examples:
						Redis: In-Memory based Database with Disk Persistence that is often used as a Caching Layer on top of other RDBMSes.

			Data Warehouse


	Commands:
		
	
