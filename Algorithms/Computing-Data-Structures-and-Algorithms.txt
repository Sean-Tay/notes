# Data Structures: Data organized in a specific way to support efficient Insertion, Searching / Updating, and Removal of Data in Memory. Contrast with DMBS for efficient storage in Disk.
	Linked List: 
		- Time Complexities:
			- Search / Update: O(n)
			- Insertion: O(n)
			- Deletion: O(n)

		- Properties: vs. Arrays:
			Linked List can dynamically adjust their Lengths, whilst Arrays have Fixed Lengths.
				Elements need not be of the same size.
			
			Array Elements can be accessed in Constant Time due to their Contiguous Nature.
				However, Linked List Nodes can also be accessed in O(1) Time if the context that is interested in the Node stores the Reference to the Node. If it is a Doubly Linked List, then Insertion and Deletion can actually be O(1) Time as well.

	Queue: FIFO Data Structure.

	Stack: LIFO Data Structure.

	Hash Table: Maps Keys to the Values they represent via a Hashing Function that efficiently determines Value location within the Data Structure.
		- Time Complexities:
			- Search / Update: Amortized O(1)
				- findMin: O(n)
				- findMax: O(n)
			- Insertion: Amortized O(1)
			- Delete: Amortized O(1)
		
		- Properties: 
			- Hash Function: A function that 'chops' up the domain into many sub-domains (i.e. definition of 'Hash').
				- Fast Computation.
				- Even Key Scatter.
				
				- Uniform Hash Function: Distributes Keys evenly in a Hash Table.
				- Perfect Hash Function: Achieves a one-to-one mapping on all given Keys (GNU gperf - Perfect Hash Function generator).
			
			- Load Factor: Num of Keys / Num of Buckets in Table
				- A Metric that represents fullness of the Hash Table.
				- A Metric for determining when the Hash Table should be resized.
				
				- If Load Factor is too low, it results in Wasted Memory.
				- If Load Factor is too high, Table becomes slower in general.
			
			- Collision Resolution: Resolution for conflicting Keys.
				- Separate Chaining:
					- Store Collisions together in a separate Data Structure associated with the Key.
					
				- Open Addressing: 
					- Linear Probing:
						- Upon Collision, scan through the Table in constant Strides to search for the next available Empty Space.
						- Caution: Beware of Deletion - use Lazy Deletion instead to prevent incorrect conclusion.
						
						- Primary Clustering: When many consecutive occupied slots are used.
						- Optimization: Stride a (consistent) number of slots instead of just 1 slot per check. Make Stride Size coprime to Array Size (no Common Factors between Two Numbers) in order to cover all slots in the Array.
						
					- Quadratic Probing:
						- Upon Collision, scan through the Table with increasingly larger steps to search for the next available Empty Space.
						- Caution: Beware of Deletion - use Lazy Deletion instead to prevent incorrect conclusion.
						
						- Secondary Clustering:	Occurs when many entries in the given Input Sequence have the same starting point.
						
					- Double Hashing:
						- Extension of Linear Probing. Use Primary Hash Function to determine starting point, followed by a Secondary Hash Function to determine Stride Size for each Key.
						- Caution: Secondary Hash Function cannot return a 0.

				- Coalesced Chaining: A combination of Seperate Chaining and Open Addressing (Linear Probing) - rather than utilizing a separate Data Structure at each Bucket, use the Buckets in the HashTable to additionally track the location of the next Element in the Collision Chain. This is better than Separate Chaining as the Table no longer has a chance to never be completely filled.

	Binary Heap: 
		- Time Complexities:
			- Search / Update: O(n) for general Node
			- Insertion: O(logn)
				- Heapification: Create a Heap with a given sequence of Elements: O(n): Execute shiftDown() on all Non-Leaf Nodes, from the lowest level to the top.
			- Deletion: O(logn)

		- Properties: 
			- Max Heap: Parent is always larger than or equal to both Child Nodes ; Min Heap: Parent is always smaller than or equal to both Child Nodes.
			- Complete Binary Tree: Every Level of the Tree is filled, except possibly the last - Nodes on the last level are all compacted towards the 'left' of the Tree.
			
			- Array Implementation: The 0th Index is not used.

			- Utility Operations:
				- parent(i): floor(i/2)
				- leftChild(i): i*2
				- rightChild(i): i*2 + 1

				- shiftUp(i): Max Heap / Min Heap: if parent(i) < i / parent(i) > i; swap parent(i) with i
				- shiftDown(i): Max Heap / Min Heap: if i < larger Child / i > smaller Child; swap i with larger Child / smaller Child

			- Insertion: 
				1. Insert into index corresponding to heapSize. 
				2. Execute shiftUp on inserted Node until Heap Property is no longer violated.
			
			- Deletion: 
				1. Extract Root, set Node at index corresponding to heapSize as root instead.
				2. Execute shiftDown on new Root until Heap Property is no longer violated.

			- Non-Leaf Node Starting Index: parent(heapSize)

		- Finds use in Priority Queues.

	Binary Search Trees: A Tree which stores lesser Data in a left Sub-Tree, and greater Data in a right Sub-Tree.
		- Time Complexities:
			- Search / Update: O(n) for non-self-balancing variant / O(logn) for self-balancing variant
			- Insertion: O(n) for non-self-balancing variant / O(logn) for self-balancing variant
			- Deletion: O(n) for non-self-balancing variant / O(logn) for self-balancing variant

		- Properties:
			- Each Node stores one Element, with at most Pointers to 2 Child Nodes.
				- The left Child Node stores lesser Elements.
				- The right Child Node stores greater Elements.

				- Threaded Binary Tree: No Null Pointers for non-Minimum and non-Maximum Nodes; a Node with no Left Child / Right Child Node will link back to the Predecessor Node / Successor Node.

			- Sorted Order:
				- Although the Nodes are of different heights, the sorted order of the Elements in the Tree can be seen by simply looking from left to right.
					- In a given [Sub-]Tree, the Leftmost Node is the smallest Element; the Rightmost Node is the largest Element.

			- Utility Operations:
				- findMin(Node) / findMax(Node): Just Leftmost or Rightmost Node of [Sub-]Tree rooted at Node.
					
				- findPredecessor(Node) / findSuccessor(Node): 
					1. Given Node has corresponding Child Node: findPredecessor ; findMax(Left Child) / findSuccessor ; findMin(Right Child)
						- Note: If using Moris Traversal / Threaded Binary Trees, terminate when the Descendent links back to Node.
					2. Given Node has specific Ancestor: 'Left' Ancestor ; findPredecessor / 'Right' Ancestor ; findSuccessor
				
				- size(Node): Utilize BFS / Level-Order Traversal to obtain size of [Sub-]Tree[s] rooted at Node.
					- Size of Empty Tree == 0
					- Size of Node == size(Left Sub-Tree) + size(Right Sub-Tree) + 1

				- rank(Node): size(Left Sub-Tree) + 1

				- height(Node): max(height(Left Sub-Tree), height(Right Sub-Tree)) + 1
					- Height of Empty Tree == -1
					- Height of Leaf Node == 0

			- Deletion: 
				1. If the Node is Childless, just remove.
				2. If the Node has one Child, just bypass current Node to the Child Node directly from the Parent.
				3. If the Node has two Children, replace Node with Predecessor / Successor key, and delete Predecessor / Successor Node.
					- Tip: Predecessor / Successor of a Node with 2 Children guaranteed to either only have zero or one Child.

			- Balance Factor: height(Left Sub-Tree) - height(Right Sub-Tree)
				- A Metric for determining if a given [Sub-]Tree is adequately not-degenerate.
				- If Balance Factor is < - 1 or > 1, then the [Sub-]Tree is not balanced, and at risk of becoming degenerate upon further operations.

			- AVL Trees:
				- Rebalancing: Rotations to prevent degenerate Trees.
					- Types:
						left_rotate(Node): Make Right Child Parent of Node, transfer Left Sub-Tree of Right Child to become Right Sub-Tree of Node.
						right_rotate(Node): Make Left Child Parent of Node, transfer Right Sub-Tree of Left Child to become Left Sub-Tree of Node.

					- Upon Insertion / Deletion of a Node, traverse back up the Tree all the way to the Root.
					- Upon encountering an unbalanced Ancestor Node (Balance Factor == 2 or == -2 at Node), execute one of the four cases:
						1. Balance Factor 2: LEFT Ancestor Node's Child and LEFT Ancestor Ancestor's Node Child: right_rotate(Ancestor Node)
						2. Balance Factor -2: RIGHT Ancestor Node's Child and RIGHT Ancestor Ancestor's Node Child: left_rotate(Ancestor Node)
						
						3. Balance Factor 2: LEFT Ancestor Node's Child and RIGHT Ancestor Ancestor Node's Child: left_rotate(LEFT Ancestor Node's Child), then Case 1: right_rotate(Ancestor Node)
						4. Balance Factor -2: RIGHT Ancestor Node's Child and LEFT Ancestor Ancestor Node's Child: right_rotate(RIGHT Ancestor Node's Child), then Case 2: left_rotate(Ancestor Node)

						- At the end of the operation, the Ancestor Node will have a Balance Factor of 0.

					- Pretty good Balancing Mechanism, but will result in more Rotations.

			- Red-Black Trees: A less balanced BST, but less Rotations. Good for frequent Insertions and Deletions.

	UFDS: Union-Find Disjoint Set: A Data Structure which tracks the membership of a Set of Elements partitioned into smaller non-overlapping disjoint Sub-Sets.
		- Time Complexities:
			- Search / Update: O(Inverse Ackermann Function)

		- Properties:
			- [Sub-]Sets here can be visually imagined as m-way Trees.

			- Each Element has an accompanying:
				- 'Parent' Status: The value here is the index of a 'Parent' Element. If the 'Parent' Element is itself, it is considered the representative Element of a [Sub-]Set. At creation, an Element's Parent is itself (it is the representative Element of its own Set).
				- 'Rank' Status: An *upperbound* metric that represents the 'height' of the [Sub-]Tree rooted at the given Element. At creation, an Element's Rank is 0. It is used to help keep the Trees short during Union Operations.

			- Utility Operations:
				- findSet(Element): Using the 'Parent' Status of each Element, recursively visit Ancestor Elements until the representative Element of the [Sub-]Set the Element belongs to is found. To keep the future search time low:
					- Path Compression: The 'Parent' of the Element is fixed to the representative Element of the encompassing Set once it is determined.
					- Path Halving: The 'Parent' of the Element and *every other* Ancestor Element is fixed to their corresponding Grandparent Elements.
					- Path Splitting: The 'Parent' of the Element and Ancestor Elements are fixed to their corresponding Grandparent Elements.

				- unionSet(ElementX, ElementY): Joins the encompassing [Sub-]Sets of ElementX and ElementY together.
					1) repX = findSet(ElementX); repY = findSet(ElementY);
					2) Set the 'Parent' of either repX / repY to be repY / repX.
						- The representative Element with the smaller 'Rank' will be set as the Child of the other representative Element.
						- If the two representative Elements have the same 'Rank', the ultimate representative Element of the two will have the 'Rank' incremented by 1.

	Bitmask: A Data Structure which tracks some status of an Array of Elements.
		- Time Complexities:
			- Search / Update: O(1) (Bit Shift Complexity dependent on CPU Processor implementation)

		- Properties:
			- A Number whose n Bits tracks n Elements.
				- Each Bit can represent a Boolean Value of 0 or 1.
				- Integer: 32 Bits = 32 Elements.
			
			- Utility Operations:
				- Check ith Bit: x & (1 << (n - (i + 1)))
				- Activate ith Bit: x = x | (1 << (n - (i + 1)))
				- Activate ith Bit to jth Bit inclusive: x = x | ((1 << (n - (i + 2)) - 1) & (1 << (n - (j + 1)))

	Graphs:
		- Terminology:
			- Vertex: Node
				- In / Out Degree: The number of Edges leading in / out to / from the Vertex.

			- Edge: A Link between two Vertices.
				- Direction
				- Weight

				- Two Vertices are adjacent if they are linked by an Edge.
				- A Vertex is incident to an Edge if the Vertex is one of the two the Edge links.

			- Path: Sequence of Vertices adjacent to each other.
				- Simple Path: When the Path does not contain repeated Vertices.

				- Cost:
					- Unweighted Edges: Number of involved Edges in the Path.
					- Weighted Edges: Sum of the Weights of all involved Edges.

			- Cycle: A Path which starts / ends on the same Vertex.
				- Simple Cycle: A Simple Path with a Cycle.

			- Component: A group of Vertices that can visit each other via some Path.

		- Types:
			- Undirected Graph: A Graph where all Edges are bi-directional (i.e. if there is a link from one Vertex to another, there is also a link from the other Vertex back to the first).
			- Directed Graph: A Graph where all Edges have direction (i.e. if there is a link from one Vertex to another, it does not mean that there is also a link from the other Vertex back to the first).

			- Simple Graph: Where all pairs of Unique Vertices have at most one Edge between them.

			- Sparse Graph: A Graph with few Edges.
			- Dense Graph: A Graph with many Edges.
			- Complete Graph: A Graph where every pair of Unique Vertices has an Edge between them.

			- Connected Graph: When the entire Graph is one Component.

			- Directed Acyclic Graph: A Directed Graph with no Cycle.

			- Tree: An (Undirected) Graph where there only exists a single Path between any two Vertices. Number of Edges = Number of Vertices - 1.
				- Minimum / Maximum Spanning Tree: A Tree covering all Vertices in the overall Graph, and whose Cost is the minimum / maximum possible.

			- Bipartite Graph: A Graph where all Vertices can be partitioned into two Sets such that all Vertices in each Set cannot directly visit another member of the same Set.

		- Representative Data Structures:
			- Adjacency Matrix: A 2D Array which stores every link between every possible pair of Vertices in the Graph.
				- Space Complexity: O(V^2)
				
				- Good for Dense Graphs.

			- Adjacency List: An Array of Lists, in which each List stores only adjacent Vertices for the Vertex it represents.
				- Space Complexity: O(E + V)

				- Good for Sparse Graphs.

			- Edge List: A List that tracks all adjacent Vertex Pairs.
				- Space Complexity: O(E)

	B Trees: An extension of a Binary Search Tree - m-way Search Tree, but with constraints on how new Elements are inserted to prevent degenerate Trees.
		- Time Complexities:
			- Search / Update: O(logn)
			- Insertion: O(logn)
			- Deletion: O(logn)
		
		- Properties: 
			- Each Node can now store up to m-1 Elements, and m references to Children Nodes (m Degree / Order).
				- References to Child Nodes can be visualized as being stored 'between' the Elements.
				- Element Order in each Node is important.
			
			- Self-Balancing Properties:
				- All Non-Root Nodes with Children should have at least ceil(m/2) Child Nodes.
					- A Child Node can only be created if this property is fulfilled for the given Non-Leaf Node.

				- All Leaf Nodes should have at least ceil(m/2) - 1 Elements.

				- All Leaf Nodes (Nodes with no Children) should be at the same Level.
					- (Proactive) Insertion: Advantageous of not visiting a Node twice.
						- Bottom-Up: Tree grows upwards ; actual Element Insertion should only occur at a Leaf Node.
						- Traverse down from the Root Node to a Leaf Node.
							- If a given Node during Traversal is full, split the Node into 2 Child Nodes, and take the median Element from the original Node and set it to a Parent Node.
								- Left Bias: Resultant Left Child will have more Elements.
								- Right Bias: Resultant Right Child will have more Elements.

							- Continue Traversal from one of the Child Nodes.
					
					- (Proactive) Deletion: 
						- Traverse down from the Root Node.
							- If Element is in Non-Leaf Node:
								- If the Child Node that precedes / succeeds the Element has at least ceil(m/2)-1 Elements:
									- Obtain the Predecessor / Successor of the Element, depending on which Child Node has enough Elements.
									- Replace Element with Predecessor / Successor Element.
									- Delete Predecessor / Successor - guaranteed to be in Leaf Node.
									
								- Else: Both Child Nodes has less than ceil(m/2)-1 Elements:
									- Merge Both Child Nodes together, set Element to be Median Key of Child Node.
									- Traverse to new Child Node, and recursively delete Key from Child Node.

									- Note: Parent may have less than ceil(m/2)-1 Elements as a result of this operation, but this will be resolved in subsequent Deletes.

							- Else: Element is not in Current Non-Leaf Node.
								- Determine next Child Node to traverse to.

								- If Child Node has less than the minimum ceil(m/2)-1 Elements:
									- If an immediate Sibling Node has at least ceil(m/2) Elements, execute a left or right Rotation on one of the neighbouring Parent Elements.
										- Send the neighbouring Parent Element down to the Child Node.
										- Set the Predecessor or Successor Element of the Sibling Node in the Parent Element's position.
									- If both immediate Sibling Nodes have less than ceil(m/2) Elements, merge one of the Siblings together with the Child Node, with one the Parent Element becoming the Median Element of the Child Node.
									
								- Continue Traversal.
								- If Traversal has reached a Leaf Node, delete Element from Leaf Node if it exists.
	
		- Finds use in Multi-Level Database Indexes.
			- Leaf Nodes - Dense Index.
			- Non-Leaf Nodes - Sparse Indexes to the Blocks containing the Dense Index.

	B+ Trees: An extension to B Trees. Can be viewed as a Tree + Linked List combination.
		- Properties:
			- Does not need to traverse the whole Tree for Sequential Access:
				- All Non-Leaf Nodes have their Elements in the Leaf Nodes as well.
				- All Leaf Nodes will be connected together like a Linked List from left to right.

	Segment Tree

	Fenwick Tree

	Trie: Tree Data Structure that is used to store collections of Sequences with no repeated sub-Sequences.
		- Time Complexities:
			- Search: O(l), where l is the average length of the Sequences to be Searched.
			- Insertion: O(n * l), where n is the number of Sequences to be Inserted, and l is the average length of the Sequences to be Inserted.
			- Deletion: O(n * l), where n is the number of Sequences to be Deleted, and l is the average length of the Sequences to be Deleted.

		- Properties:
			- Each Node should store:
				- A Map of Child Characters to Child Nodes / Array of Child Nodes in Lexicographic Order of the Characters, if all Characters are known.
					- Default: Empty Map / Array of Nulls.

				- A 'Complete Sequence' boolean that indicates if the Path to this Node is a given Sequence.
					- Default: false.
					- Note: The Nodes whose Complete Sequence boolean is set to true may / may not have Children.
						- No Children: Leaf Node / Complete Sequence.
						- Children: Complete Sequence which is a sub-Sequence of another.

				- Intuitively / Visually, the Character that a Node represents is stored at its Parent Node.
					- Traverse down first, then check Complete Sequence Boolean.

			- Search:
				- Primarily for Prefix-based Searching.

				- Starting from the Root Node, for each Character in the Sequence to be Searched:
					 - Is there a Child Node that contains the Current Character?
					 	- If not:
						 	- Sequence does not exist in the Trie.

						- Else:
							1. Traverse down to the corresponding Child Node.
								- If the Character is the last in the Sequence:
									- Check the Child Node's Complete Sequence boolean:
										- If false, Sequence exists as a Prefix in the Trie, but it was not inserted into the Trie.
										- If true, Sequence exists within the Trie.

								- Else, move on to the next Character in the Sequence to be Searched, repeating the above.

			- Insertion:
				- Starting from the Root Node, for each Character in the Sequence to be Inserted:
					- Is there a Child Node that contains the Current Character?
						- If so:
							1. Traverse down to the corresponding Child Node.
								- If the Character is the last in the Sequence:
									- Set the corresponding Child Node's Complete Sequence boolean to be true.

								- Else, move on to the next Character in the Sequence to be Inserted, repeating the above.

						- Else:
							1. Create a new Child Node with the Current Character.
							2. Track the Current Character - Child Node Mapping in the Current Node.
							3. Traverse down to the new Child Node.
								- If the Character is the last in the Sequence:
									- Set the corresponding Child Node's Complete Sequence boolean to be true.

								- Else, move on to the next Character in the Sequence to be Inserted, repeating the above.

			- Deletion: Use Recursion or a Stack to track the Nodes traversed.
				- Starting from the Root Node, for each Character in the Sequence to be Inserted:
					- Is there a Child Node that contains the Current Character?
						- If so:
							1. Push the Current Node into the Stack.

							2. Traverse down to the corresponding Child Node.
								- If the Character is the last in the Sequence:
									- If the Child Node has Children:
										- Set the corresponding Child Node's Complete Sequence boolean to be false.

									- Else:
										- For each Node in the Stack:
											- Note: The Stack does not contain the last Node in the Sequence.

											- Remove the Character - Child Mapping from the Node.
												- If the Node has no more Children as a result, delete the Current Node and move on to the next Node in the Stack.
												- If the Node still has Children, stop here.

								- Else, move on to the next Character in the Sequence to be Deleted, repeating the above.

		- Good for Prefix-Based Searching.
			- Valid Prefix.
			- Autocomplete.

	Bitmask

# Algorithms
	Analysis
		- Bound Notation:
			- Big Omega: Represents the Lower Bound of a given Growth Rate. The Lower Bound may be lesser than the actual Growth Rate (Better than Expected).
			- Big Theta: Represents both the Upper and Lower Bound (Tight Binding) of a given Growth Rate. Usually mixed with Big O.
			- Big O: Represents the Upper Bound of a given Growth Rate. The Upper Bound may be greater than the actual Growth Rate (Worse than Expected).
	
		- Complexity Hierarchy: O(1) < O(log(logn)) < O(logn) < O((logn)^k) < O(n^(k less than 1)) < O(n) < O(nlog*n) < O(nlogn) == O(log(n!)) < O(n^(k more than 1)) < O(n^(log(n))) < O(k^n) < O(n!) < O(n^n)
			- Common Formulae:
				- Sum of Arithmetic Series: 	1 + 2 + ... + (n-1) + (n) => n(n-1) / 2
	
		- Amortized Analysis: Overall Algo Behaviour when not considering infrequent, extreme bottlenecks (the Average Case Scenario).
			- Example: Adding an Element to an Array -> O(1) usually, occasional O(n) when expanding Array.
		- Asymptotic Analysis: Algo Behaviour when considering increasingly large inputs (the Worst Case Scenario).

		- Space Complexity: How much Memory is required for the Algorithm.
			- Stack Space for Recursive Calls count as well.
	
	Array:
		- Circular:
			- Head Pointer and Tail Pointer.
				- If both pointing to same Index, Array is Empty.
				- When full, Head and Tail Pointer should be separated by one empty Index to ensure no confusion with Empty state.

		- Two Pointers / Sliding Window	

		- Tip: 
			- For frequent lookups to Unsorted Arrays, consider converting the Array to a Map (Element-Index) first.
			- For frequent lookups to (already) Sorted Arrays, consider using Binary Search.

			- If order of Elements are unimportant, Deletion need not be O(n):
				- Swap the Element to be Deleted and the Last Element, then just pop it off.
	
	String Manipulation:
		- Regex.

		- String Comparison is not O(1).
			- Sorting String Array: each Comparison may take O(s) Time.
	
	Recursion:
		- Typically:
			1) Identify the Base-Case of the Problem / Break the Problem into smaller Sub-Problems.
			2) Call Function recursively to resolve Sub-Problems.
			3) Use results from (2) to resolve actual Problem.
		
		- Tail Recursion: When the Recursion is the last operation in a Recursive Call, the Call Stack is not kept unnecessarily.
	
	Sorting:
		Comparison Based Sorting:
			Selection Sort:		We select for the Minimum / Maximum Element per Iteration.

			Insertion Sort:		We select the next Element in each Iteration, and decide where to put it within an accumulating Sorted List.

			Bubble Sort:

			Merge Sort:			We split up the Input List, then combine the Lists, whilst sorting them.
				Stable, Not-In-Place Sort.

			Quick Sort:			
				Non-Stable, In-Place Sort.

				Key Idea: Given a Pivot Element, the Iteration is complete when Elements toward the left of the List is smaller than the Pivot, and Elements toward the right of the List is greater than the Pivot Element.

				For a given List:
					If the List Length < 1, terminate execution and return the original List.

					Select any Element from the List to be the "Pivot" for this Iteration.
						The choice of "Pivot" will affect the Performance, given different Data Patterns. Best to use the Middle Element as the Pivot.
					
					Hoare Partition Scheme:
						Initialize two Indices at each end of the List.

						Loop:
							Start moving the "Left" Index, and stop if it encounters an Element greater than or equal to the Pivot.
							Start moving the "Right" Index, and	stop if it encounters an Element smaller than or equal to the Pivot.

							Swap the two Elements at the Indexes, then start the movement of the Indexes again, with the same rules in mind as before.

						When the Indexes meet, it should be at the Pivot Element. The Pivot Element's Index will be the Index at where the List should be split.
							Note: The Pivot Element may change position during the Iteration, especially if there are Repeated Pivot Elements.
					
					Split the List, then recursively process the Sub-Lists.
			
			Heap Sort:
		
		Non-Comparison Based Sorting:
			Radix / Base Sort:
				- Radix: The smallest number that can only be represented by more than one symbol.

			Counting Sort:

			Bucket Sort:
	
	Binary Search:

	BFS: Graph Traversal with a Queue, and a 'visited' Array which tracks if the Vertex has been processed before in this traversal.
		- Time Complexity: O(V+E)

		- If an additional 'predecessor' Array is used to track the vertex that was visited before its adjacent vertices, the Graph Traversal can generate a Spanning Tree, represented by this Array.

	DFS: Graph Traversal with a Stack, and a 'visited' Array which tracks if the Vertex has been processed before in this traversal.
		- Time Complexity: O(V+E)

		- If an additional 'predecessor' Array is used to track the vertex that was visited before its adjacent vertices, the Graph Traversal can generate a Spanning Tree, represented by this Array.

	Tree Traversal:
		- DFS: Key Idea is to push Nodes onto the Stack to access them later.
			- Pre-Order: Idea: Can push multiple Nodes into the Stack at one time.
				1. If Current Node is not null, visit Current Node, then push Right Child, then Left Child, into Stack.
				2. Pop from Stack to descend.
				3. Iterate until Stack is empty.

				- Root is always visited first.

			- In-Order: Idea: Push Nodes onto the Stack to access them later.
				1. If Current Node is not null, push Current Node into Stack, then traverse to Left Child.
				2. If Current Node is null, pop from Stack, visit the popped Node, then descend to Right Child.
				3. Iterate until Stack is empty and Current Node is null.

				- Leftmost Node is always visited first.

			- Post-Order: Ideas: Can push same Node more than once / Can 'rearrange' pushed Nodes.
				1. If Current Node is not null, push Current Node into Stack twice and traverse to Left Child.
				2. If Current Node is null, pop from Stack.
					a. If the topmost Node in the Stack is the same as popped Node, traverse to the Right Child.
					b. Else, visit popped Node.

				- Root is always visited last.

		- Morris Traversal: If the Current Node has a Left Child, find the In-Order Leaf Predecessor of this Current Node. The Leaf Predecessor's Right Child is used as an indication as to whether or not the Current Node has been accessed before:
			- If the Leaf Predecessor's Right Child is null, it means the Current Node has not been accessed before. Set that Leaf Predecessor's Right Child to point to the Current Node, and descend. This also allows the Traversal to eventually reach back to this Current Node.
			- If the Leaf Predecessor's Right Child points to the Current Node, then this Current Node has been visited before.
		
			- Time Complexity: O(n) - Num of Edges is n-1, each Edge visited at most 3 times.

		- Level-Order: BFS on a Tree

	Topological Sort: When performed on a DAG, for every Vertex, they are processed only after every other Vertex leading into them is processed.
		- Finds use in Task Scheduling - if some Tasks are dependent on other Tasks.
			- Package Dependency Resolution.

		- A Graph can have more than one valid Topological Ordering.

		- DFS, but only visit the Current Vertex after adjacent Vertices have been visited (similar to Post-Order Traversal).
			- The method will output the Topological Ordering in reverse order.

	Minimum / Maximum Spanning Tree: A Tree covering all Vertices in the overall Graph, and whose Cost is the minimum / maximum possible.
		- Prim's: 		BFS, but with a Priority Queue acting on the Edge Weight as opposed to a regular Queue.
			- Time Complexity: O(E log(V))

		- Boruvka's: 	In each iteration, find Vertices / Components in the Graph, and add the cheapest Edges incident to each Vertex / Component that links to a different Vertex / Component. 
			- Time Complexity: O(E log(V))

		- Kruskal's: 	Sort the Edges (in an Edge List) in Ascending / Descending Order, and on each Edge determine if it can be added to the overall Tree without forming a Cycle (use UFDS to determine if two vertices already have the same Parent).
			- Time Complexity: O(E log(V))

	Shortest Path
		- Dijkstra
		- Floyd Warshal
		- Bellman Ford?
		- A*

	Flood Fill

	Greedy
	
	Divide and Conquer: Involves Problems that have discrete Sub-Problems.

	Dynamic Programming: Involves Problems that have overlapping Sub-Problems.
		1) Top-Down: Recursion / Memoize
			- Avoids recomputing some Past States in overlapping Sub-Problems.
			- Use Dictionaries for the Memoization?
			
		2) Bottom-Up: For Loops
			- Avoids Recursion Overhead.
			- Use Multidimensional Arrays for the Memoization?

	Exhaustive Search

	Backtracking: Technique for solving Problems recursively by building the Solution incrementally one piece at a time, but also stopping the building up of Solutions that fail to meet the requirements (i.e. "Backtracking" on the Solution).

